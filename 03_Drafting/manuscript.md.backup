## Precision Medicine Using Data From Multiple Studies

> [!question]- How much data does prescision medicine require? (status:: undefined)
> Precision medicine relies on accurate predictions for patients across the spectrum of human diversity. This diversity is best captured and accounted for using large datasets acquired at great cost. [source:: C_1770304648471, C_100]

> [!question]- How can dataset size be increased? (status:: undefined)
> When large sample sizes cannot practically be acquired, adding data from independent cohorts has the potential to improve dataset diversity and statistical power. These benefits convey to learners (be they human or machine) increased ability to detect patterns and generalize to new data.

> [!question]- What is the central challenge to combining datasets? (status:: undefined)
> This potential is frequently undermined by "batch effects"—systematic technical variations that can lead a learner to distinguish between experimental batches rather than meaningful biological conditions. Batch effects can substantially degrade predictor performance when applied to new individuals.

> [!question]- What is the chapter's scope in addressing these challenges? (status:: undefined)
> This chapter examines how batch effects impact predictor performance and explores statistical adjustment methods designed to mitigate these artifacts while preserving biological signal. As these adjustment methods are designed to assist in generalization to new data, emphasis is placed on using external data across independent cohorts for validation of these methods.



## Classification in Precision Medicine

> [!question]- To what extent is classification relevant to precision medicine, and what is it? (status:: undefined)
> Classification—the task of assigning observations to predefined categories based on their features—is vital to biomedical research and the implementation of precision medicine.

> [!question]- Why is classification useful for biomedical tasks? (status:: undefined)
> For instance, machine learning classifiers have demonstrated strong performance for cancer classification tasks using gene expression data, several having been approved and implemented clinically to predict personalized recurrence risk for breast cancer and guide chemotherapy decisions. [source:: C_05, C_1770403547313(Buus2021)]



## Batch Effect Sources Across Data Types

> [!question]- What data is used for biomedical classification? (status:: undefined)
> Biomedical classification tasks are not limited to gene expression, but can utilize diverse data types, including protein abundance measurements, genomic sequences, clinical variables, and imaging data. Each of these data modalities provides complementary information about biological systems and disease mechanisms. [source:: C_05]

> [!question]- How do batch effects sources vary by data type? (status:: undefined)
> No modality is immune to batch effects, though many simply measured clinical variables, such as weight, are resilient. Batch effects arise from differences in experimental protocols, equipment type and condition, reagent lots, environmental conditions, human error, and other technical factors that vary between studies or even within studies over time. Greater complexity and subjectivity in measurement proceedure generally implies greater succeptibility to batch effects. [source:: C_01, C_02]

> [!question]- What type of data will this chapter focus on? (status:: undefined)
> For our purposes, and without loss of generality, we will focus on RNA sequencing (RNA-seq) data, which has become the dominant technology for measuring gene expression. [source:: C_31]

> [!question]- Why gene expression? (status:: undefined)
> Gene expression data provide an excellent context for understanding how batch effects can be modeled and removed, as the technical variation introduced by sequencing technologies is well-characterized and substantial. Insights from gene expression are extensible to other modalities. [source:: C_07]



## Focus on Gene Expression Data

> [!question]- How are gene expression data generated and used within biological/biomedical research? (status:: undefined)
> Gene expression data are generated through high-throughput sequencing technologies that quantify the abundance of RNA transcripts in biological samples. These measurements provide a snapshot of cellular activity, reflecting which genes are actively transcribed under specific conditions, disease states, or in response to treatments. Gene expression data enable the identification of disease-associated pathways, the discovery of therapeutic targets, and the development of diagnostic and prognostic biomarkers. [source:: C_02]

> [!question]- How can researchers access gene expression data? (status:: undefined)
> The Gene Expression Omnibus (GEO) serves as a critical international public repository for such data, with over 200,000 studies and 6.5 million samples. For each molecular sample, data submitters also provide relevant clinical variables, enabling associations between molecular signatures and clinical phenotypes. GEO gives researchers access to multiple datasets.  [source:: C_31]

> [!question]- How is public gene expression data used? (status:: undefined)
> [source:: C_35]

> [!question]- Why integrate gene expression datasets? (status:: undefined)
> Integrating gene expression data across multiple studies offers substantial benefits over single-study analysis: meta-analyses can reveal consistent patterns across diverse populations, increased sample sizes improve robustness to noisy data, and independent validation cohorts provide evidence of generalizability to further populations.

> [!question]- How sensitive is gene expression to batch effects? (status:: undefined)
> However, batch effects pose particular challenges for gene expression data due to the sensitivity of sequencing technologies to technical variation. These technical artifacts are often comparable to or larger than the biological signals of interest, making them a primary concern when combining datasets. [source:: C_07]

> [!question]- What specific effects do batches have on gene expression? (status:: undefined)
> These artifacts of data collection manifest in gene expression data across batches as systematic shifts in expression levels, differences in variance structure, and even alterations in the relationships between genes. [source:: C_07]

> [!question]- What about changes to distributional shape? When might this arise? (status:: undefined)
> Batch effects can also manifest as changes in distributional shape for features shared between datasets. For gene expression data, this difficulty arises in combining high throughput sequencing (RNA-seq) data and measurements obtained using microarrays, an older technology. Across samples, microarray measurements of single-gene expression tend to be bell shaped, or normally distributed, taking on continuous values. RNA-seq measurements are discrete counts, with heavily right-skewed distributions. These right-skewed distributions can be modified using a log-based transformation to yield distributional shapes comparable with microarray data.

> [!question]- What about shifts and scaling? (status:: undefined)
> Shifts in expression levels and differences in variance structure are both addressed in the following section.



## Adjusters for Gene Expression

> [!question]- What are adjusters? (status:: undefined)
> Statistical adjustment methods, also referred to as adjusters or batch correction methods, aim to remove technical variation while preserving biological signal.

> [!question]- How do adjusters work? (status:: undefined)
> Adjusters can work by modeling the technical batch effects and removing them, or by modeling the biological space and removing all other variation. This has implications for the preservation of biological signal. If an adjuster that models biology fails to capture some variation, then that signal will be removed from the data. If an adjuster that models batch effects attributes too much variation to the batch, the same problem occurs. These methods operate under the assumption that the underlying biological signal is consistent across batches, and that technical variation can be separated from biological variation. Unbiased noise may also add variation which may be safely removed.

> [!question]- Why do some adjusters work better in some contexts? (status:: undefined)
> The effectiveness of adjusters depends on viability of the assumptions made (whether about batch effects or biology) which vary by data modality. Differences in modeling assumptions are evident in the following comparison.

> [!question]- How can gene expression measurements be categorized? (status:: undefined)
> Among the subtypes of gene expression measurements, two are dominant: bulk, which measures expression in a large sample of cells, and single cell, which measures the expression of individual cells.

> [!question]- How does Combat adjust bulk data? (status:: undefined)
> For bulk data, a common adjusting approach is ComBat. ComBat adjusts for batch effects by modeling batch-specific shifts and scaling factors for each gene. The modeled batch effect is then removed. The transformation that ComBat applies is linear: each expression value for a particular gene is shifted and scaled the same as each other expression value for that gene. This transformation works well for bulk gene expression data, which varies continuously due to the random proportions of cell types represented in the sample. [source:: C_01]

> [!question]- What is used to adjust single cell data? (status:: undefined)
> Single cell data requires a different kind of adjustment. Single cell gene expression is characterized by distinct expression signatures for each cell type, with internal variation caused by various cell states at measurement time.  If bulk data spans a continent of continuous variation, single cell data occupies islands of specialized differentiation.

> [!question]- How is single cell data adjusted? (status:: undefined)
> Single cell adjusters may assume that samples of similar cell types will exhibit similar expression patterns, even in the presence of batch effects. By finding "nearerst neighbors", or the samples with the most similar expression between batches, adjusters such as Harmony or Seurat can identify how to move cross-batch neighbors closer together to bring the batches into alignment. Harmony and Seurat model the biological space, then eliminate the batch differences. Since neighbors are found using many genes, and the transformation is sample-specific, the adjustment of single cell data is typically nonlinear.

> [!question]- Feature space correction vs latent space alignment? (status:: undefined)
> ComBat and the single cell methods have a difference in approach more fundamental than modeling batch or biology. They vary by space, or the variables which they work with. ComBat is a feature-space correction method: it attempts to fix the original data and output new values for each sample and gene. Harmony and Seurat are latent-space alignment methods: they represent each sample using fewer variables, which represent simple ways the data can vary within a single batch. These new variables are called latent, or hidden, because they were not among the original genes yet hold most of the information. After changing the representation, Harmony and Seurat perform their adjustments in the latent space.



## Confounding

> [!question]- What is confounding? (status:: undefined)
> Batch effects can mimic or mask real differences in expression levels and covariance structures between batches. Not all differences between datasets are due to technical artifacts; not all differences should be removed. For example, two studies might have the same split of positive and negative cases, but different proportions of female subjects. It might be difficult to determine whether the differences in gene expression between batches are technical artifacts that should be removed, or true differences due to the sex imbalance between datasets. We might say that the batch effect is "confounded" with sex—the effect of the batch on gene expression is somewhat tied up with the effect of the population imbalance.

> [!question]- How can confounding be resolved? (status:: undefined)
> Confounding can often be removed if the values of the confounding variables are known.

> [!question]- How specifically does Combat account for confounding? (status:: undefined)
> For example, ComBat, when provided with additional metadata for each sample, can temporarily remove the associations between the metadata and the gene expression, then remove the remaining batch effects, then add back the metadata associations.

> [!question]- When is confounding difficult to overcome? (status:: undefined)
> However, if these other variables are not known, due to poor recording or unknown population differences, correcting for confounding can be more difficult. In the multi-study context, these additional variables, or metadata, are rarely consistently recorded. One variable might be important to one study, but left out in another. These unshared variables are typically unable to be used for merging datasets. Even when variables are shared, their names and values are rarely standardized, which adds practical difficulty to combining datasets.

> [!question]- How can dataset differences be preserved? (status:: undefined)
> In addition, some datasets are fully confounded—one dataset could be fully healthy, and another fully diseased; one could be from human tissue, and another from mouse tissue. Even if this information is preserved in the metadata, using this information in an adjuster is difficult—in a two-study merge, fully confounded variables carry the same information as a binary variable indicating which dataset the sample came from. Some methods, like LIGER, have been developed to deal with this problem for single cell data. LIGER uses matrix factorization to identify shared and dataset-specific features of cell identity. Once differences are identified, they can be preserved, minimizing false alignment of the datasets.

> [!question]- How to deal with unknown batches? (status:: undefined)
> A separate but related problem occurs when the batches are not known. This is not a primary difficulty when combining datasets. Methods such as surrogate variable analysis (SVA) can identify and adjust for unknown batch effects by extracting surrogate variables that capture unwanted effects.

> [!question]- How important is single-patient data? (status:: undefined)
> For precision medicine, single-patient data also poses a major problem to batch correction. Single-patient data processing is vital to the translation of molecular assays, as patient samples in clinical settings are typically collected in small numbers, often one at a time. However, many correction techniques rely on several samples to characterize the distribution of the new batch. It is difficult to know if a single sample is an outlier if the distribution is not known. If at all possible, it is best to process several samples at the same time or use recent data identically collected to define the distribution of the new data.

> [!question]- What if I only have single-patient data? (status:: undefined)
> If this is not possible, some transformations are available to shift the data into a batch-independent space. This includes within-sample gene ranking and using Variational Autoencoders to encode the sample using latent variables. As shown later, the performance of within-sample gene ranking is weak compared to other methods that utilize information from multiple samples.

> [!question]- Transition (status:: undefined)
> To understand the effects of adjustment on classification, we will first describe the modern classification landscape.



## Machine Learning Classifiers for Gene Expression Data

> [!question]- ML, traditional, broad strokes, direction of field in usage (status:: undefined)
> Classification has evolved from traditional statistical methods (logistic regression, linear discriminant analysis) toward modern machine learning approaches (support vector machines, random forests, neural networks). The direction increasingly favors flexible methods that can capture complex, non-linear patterns in high-dimensional data. Modern techniques often use iterative and stochastic (random) training, improving the model in small steps to classify the training data correctly. [source:: C_05]

> [!question]- Compare stats and ML models, bias variance tradeoff (status:: undefined)
> Simple, rigid models use strong assumptions about how the inputs relate to the labels. This bias provides protection against variable data: the predicted model will not change much in response to small changes in the data. More complex machine learning models are less constrained, which can lead to highly variable model parameters. Many machine learning algorithms use some form of regularization, or assumptions of simple relationships that help to constrain the models. This can improve generalization, or the ability to classify unseen data.

> [!question]- What specific classifiers do well with gene expression? (status:: undefined)
> For gene expression data specifically, benchmark studies have identified several classifier types that perform particularly well. Support vector machines (SVM) and random forests have been highlighted as top performers. Logistic regression with regularization and neural networks (when sufficient data are available) also show strong performance. XGBoost, a tree-based algorithm with strong performance on many datasets, can also be used. Each classifier type has distinct characteristics that make it suitable for different scenarios and data types. [source:: C_06(Piccolo2022)]

> [!question]- What distinguishes geometric models in the genomic context? (status:: undefined)
> Support vector machines identify decision boundaries in high-dimensional gene expression space using geometric principles. For gene expression data, where biological classes may not be linearly separable due to complex regulatory networks and pathway interactions, the ability to learn non-linear boundaries is crucial. Support vector machines identify optimal decision boundaries by maximizing the margin between classes. SVMs are particularly effective for the few-sample problem in gene expression data because they focus on support vectors—the most informative samples near the decision boundary—rather than attempting to model all samples equally. [source:: C_86]

> [!question]- New question? (status:: undefined)
> Neural networks, including multi-layer perceptrons and more sophisticated architectures, can achieve excellent performance when sufficient data are available. For gene expression data, neural networks can learn hierarchical representations where early layers capture individual gene patterns and deeper layers integrate these into pathway-level or systems-level features. Neural networks outperform other methods only when training set sizes are very large, as the high-dimensional nature of gene expression data (many features, few samples) has historically limited deep learning effectiveness. Deep learning approaches have shown particular promise for identifying complex, non-linear patterns that may be missed by simpler methods.

> [!question]- Why is regularization useful? (status:: undefined)
> Even for simple models, regularization is vital for high-dimensional gene expression data. Unregularized logistic regression succumbs to the noise of tens of thousands of genes when training, using small contributions from many genes to fit the training data exactly.

> [!question]- What is Elastic Net? (status:: undefined)
> Elastic net accounts for this tendency by effectively limiting the number of genes the model can used. This is called feature selection. Feature selection, done manually or incorportated into models, is important whenever the number of features far exceeds the number of samples used for training. This is characteristic of gene expression data, where thousands of genes vastly outnumber samples. Elastic net uses two common types of regularization, referred to as L1 and L2 regularization. The L1 penalty drives coefficients of irrelevant genes to exactly zero, creating models that use few features. This results in interpretable and computationally efficient models.  [source:: C_79]

> [!question]- How to use each classifier to find important genes (status:: undefined)
> Feature selection can also be done post-classification by examining which features were important for classifying the training or validation data. This is more natural for some classifiers than others. Some strategies, such as permutation importance, generalize to any classifier type. [source:: C_75]

> [!question]- Why feature selection matters (status:: undefined)
> In the context of precision medicine, feature selection finds minimal diagnostic signatures. This can reduce thousands of genes to a small panel suitable for cost-effective clinical assays. However, this utility depends entirely on selecting genes that represent genuine disease biology rather than technical artifacts. A minimal signature derived from batch-confounded data will fail catastrophically when deployed in a clinical setting with different technical conditions.

> [!question]- New question? (status:: undefined)
> Generalization to new settings, a requirement of precision medicine, also requires resiliance to noisy data. Fitting training data exactly is equivalent to fitting noise that will not be found in other settings. Overcoming noise requires regularization, such as the L2 penalty.

> [!question]- New question? (status:: undefined)
> The L2 penalty penalizes large coefficients, which encodes the idea that genes or features with low variation should not have a large effect on classification. The idea that features should not be weighted to strongly is a general idea that is widely applicable to all data types. Elastic net, neural networks, and other models that utilize weights can, and often do, use L1 and L2 penalties.

> [!question]- What are ensemble methods? (status:: undefined)
> Regularization can also be incorporated by training many simple learners into a more complex ensemble model. Since each simple learner is constrained in its ability to fit the data, this imposes a bias that adds regularity to the larger model. These ensemble methods are robust to the high-dimensional, noisy nature of gene expression data because they aggregate predictions across small models trained on random subsets of samples and features. This averaging effect reduces sensitivity to outliers. [source:: C_84]

> [!question]- What are random forests? (status:: undefined)
> One popular ensemble model is the random forest. Random forests construct ensembles of decision trees, where each tree is trained on a bootstrap sample of the data and a random subset of features. For gene expression data, this means each tree sees a different combination of genes and samples, preventing small deviations from patterns from dominating the model. The final prediction aggregates votes across all trees, providing robustness to noise and the ability to capture complex interactions between genes. Random forests also provide measures of feature importance, which can aid in biological interpretation by identifying which genes contribute most to classification decisions. [source:: C_84]

> [!question]- What is XGBoost? (status:: undefined)
> A similar model, XGBoost, builds trees sequentially to correct errors from previous iterations, often achieving excellent performance on structured data. It benefits from the same regularization methods as random forests. [source:: C_81]



## The Link Between Adjustment and Classifier Performance

> [!question]- Why does classifier performance depend on adjustment? (status:: undefined)
> Batch effects can introduce systematic biases that classifiers learn to exploit, leading to inflated performance on merged training data but poor generalization. Effective batch correction removes these technical artifacts, allowing classifiers to focus on biological signals and improving external-study performance.

> [!question]- Why is cross study performance a good indicator of both biological preservation and batch reduction? (status:: undefined)
> External-study performance serves as a good indicator of both biological preservation and batch reduction because it directly tests whether a classifier can generalize to independent datasets with potentially different technical characteristics. High performance on unseen external data suggests that batch effects have been successfully removed while biological signal has been preserved.

> [!question]- What about k fold cross validation? (status:: undefined)
> K-fold cross validation on merged data would seem to offer the same benefits, but is flawed. K-fold cross validation splits the merged dataset into k groups. One group is selected as the validation group, and the classifier is trained using the other k-1 groups. After evaluation on the validation set, the process is repeated. K-fold cross validation may give optimistic performance estimates because the classifier, trained on all batches, can learn batch-specific patterns that do not generalize. In essense, while the validation groups truly represent unseen samples, they do not accurately simulate unseen batches. Whenever batch effects are possible, unseen batches must be accounted for and adequately tested against.

> [!question]- What is the proper procedure? (status:: undefined)
> Testing against unseen batches, without adjustment, usually results in failure. Exceptions occur if the unseen batch is remarkably similar to a batch used in training. In theory, if datasets representing the full joint spectrum of batch effects and biological variability were assembled, a classifier could then generalize easily to any new data. In most cases, the data to be predicted on must be adjusted to match the training set, without modification of the data used in training. This homogenizes the data used as input to the classifier. Any datasets merged to form the training sets should also be adjusted, to prevent "shortcut learning" on technical differences unrelated to biology.

> [!question]- PCA (status:: undefined)
> Principal component analysis (PCA) and other visualization methods can help identify batch effects but do not directly measure their impact on classifier performance. If points are colored or shaped by label, separation between batches indicates that biological signal needed for classification is present in the most variable genes. After adjustment, PCA often shows visual overlap between batches. Though PCA might show overlap in the two most variable directions, it cannot confirm the absence of separation in other directions.  [source:: C_15]

> [!question]- Mixing Metrics (status:: undefined)
> Neighbor mixing metrics identify the proportions of nearest neigbors that share a batch or a metadata label. Ideally, samples are well mixed by batch and poorly mixed by label. This gives some insight into whether a classifier may perform well across datasets, but does not guarantee superior classification outside of nearest-neigbor based classifiers.

> [!question]- Visualization-Based Evaluation (status:: undefined)
> Further tools for visualization of batch effects can be useful. BatchQC provides interactive software for evaluating sample and batch effects with multiple diagnostic approaches including PCA, heatmaps, dendrograms, and statistical metrics. These visualizations help researchers assess whether batch effects have been successfully removed while preserving biological structure, though these methods, especially those relying on visual separation are not conclusive. [source:: C_15]



## Datasets

> [!question]- Why were these specific TB datasets chosen? (status:: undefined)
> To rigorously evaluate the impact of batch effects on classifier performance, we selected tuberculosis gene expression datasets that represent real world noise. Rather than choosing datasets that are technically similar, we deliberately selected studies that juxtapose adolescent and adult prospective cohorts with pediatric and adult case-control studies, household contact studies with clinical diagnostic studies, whole blood samples with sputum specimens, and data collected across four continents using different sequencing platforms. If batch correction methods can preserve biological signal while removing technical artifacts across this diversity, they are likely to succeed in real-world precision medicine applications.

> [!question]- What is the common biological thread across datasets that makes them comparable? (status:: undefined)
> While most datasets focus on the classification task of distinguishing active tuberculosis from latent infection, the collection also includes studies examining TB progression risk. This diversity of biological questions, while introducing additional complexity, tests whether batch correction methods can preserve distinct biological signals across different experimental designs and research objectives. [source:: C_69, C_70]

> [!question]- What are the datasets? (status:: undefined)
> The study details are summarized in this table:
> |**Study**|**Region**|**Population**|**# Samples**|**Key Characteristics**|**GEO Identifier**|**# Active**|**# Latent**|**# Progressors**|**# Non-progressors**|**# Other**|**Technology**|
> |---|---|---|---|---|---|---|---|---|---|---|---|
> |Zak et al. (2016)|South Africa|Adolescents (12-18)|153|Longitudinal sampling every 6 months to predict progression|GSE79362|-|-|46|107|-|RNA-seq|
> |Suliman et al. (2018)|South Africa, Gambia, Ethiopia|Adults|407|Household contacts, RISK4 four-gene signature|GSE94438|-|-|79|328|-|RNA-seq|
> |Anderson et al. (2014)|South Africa, Malawi, Kenya|Children (<15)|334|Childhood TB diagnosis, 51-transcript signature|GSE39941|111|54|-|-|169|Microarray|
> |Leong et al. (2018)|India|Adults|44|South Indian population, active vs latent|GSE101705|28|16|-|-|-|RNA-seq|
> |Kaforou et al. (2013)|South Africa, Malawi|Adults (18+)|584|HIV-infected and -uninfected cohorts|GSE37250|195|167|-|-|222|Microarray|
> |Low-Endemic Comparators|USA, UK, Multinational|Adults / Mixed|440|Healthy controls and ODs used for analytical specificity testing|GSE19491, GSE42834|61|69|-|-|310|Microarray|
> 
> [source:: C_69, C_70, C_68, C_71, C_73] GSE37250_SA (South Africa), GSE37250_M (Malawi),  India, USA, and Africa cohorts.



## Classifier Performance

> [!question]- What is the key finding about classifier hierarchy? (status:: undefined)
> The results show that regularization is essential for classifying high-dimensional gene expression data.

> [!question]- Methods (status:: undefined)
> The analysis evaluated classifier performance across multiple tuberculosis gene expression datasets using leave-one-study-out cross-validation. Nine machine learning classifiers were tested: elastic net (regularized logistic regression), k-nearest neighbors (KNN), logistic regression, a three layer neural network, random forests, shrinkage linear discriminant analysis (LDA), support vector machines (SVM), and XGBoost. Ten batch adjustment methods were compared: ComBat, ComBat with mean-only adjustment, ComBat-Seq supervised adjustment, naive mean and variance matching, mutual nearest neighbors (MNN), FastMNN, nonparanormal transformation (NPN), rank-based normalization over genes within samples, the same but additionally ranked over samples, with unadjusted merged and within-study cross-validation as baselines. Performance was assessed using Matthews correlation coefficient (MCC). MCC performance is decreased if a classifier has poor sensitivity, specificity, positive predictive value, or negative predictive value, and provides a good lower bound on performance. MCC ranges from 1, perfect, to 0, random, to -1, perfectly inverted. The test study labels and descriptions are found in Table 1.

> [!question]- Results (Figure 1) (status:: undefined)
> ![Figure 1: Average Rank by Classifier](../figures/average_rank_by_classifier.png) *Figure 1: Classifier performance on cross-study tasks.*

> [!question]- Discussion on classifier complexity (status:: undefined)
> Classifier complexity relates to the model's capacity to capture patterns in the data. More complex models (e.g., neural networks) may overfit when sample sizes are small, while simpler models (e.g., logistic regression) may underfit when patterns are non-linear. The results demonstrate that moderately complex classifiers with built-in regularization (elastic net, XGBoost) achieved the best balance between model flexibility and generalization. Simple logistic regression without regularization performed poorly in this high-dimensional setting, while elastic net's L1/L2 regularization enabled effective feature selection and robust performance. The performance of shinkage LDA was highly variable, having individual performances near perfection like XGBoost, but also having a long tail of poor performances, suggesting that additional regularization perhaps could have improved performance. A simple L2-regularized neural network despite its complexity, also performed well.



## Interaction Effects Between Adjusters and Classifiers

> [!question]- Why might adjusters and classifiers have interaction effects? (status:: undefined)
> Different batch correction methods may preserve or remove different aspects of the data structure, which could interact with how different classifiers learn decision boundaries. For example, some adjusters may preserve non-linear relationships better than others, potentially favoring classifiers that can exploit such patterns.

> [!question]- Do specific classifiers do better with specific adjusters, or is performance independent? (status:: undefined)
> The analysis reveals that classifier performance is largely independent of the specific batch adjustment method used, with some notable exceptions.

> [!question]- Figure 2 (status:: undefined)
> ![Figure 2: MCC Rank by Adjuster](../figures/mcc_rank_by_adjuster.png) *Figure 2: Description*

> [!question]- What does Figure 2 reveal about the change in performance? (status:: undefined)
> Figure 2 displays results for each adjuster and classifier, aggregated over test and training sets. Using

> [!question]- Show a few places where interactions occur, but mostly independent performance (status:: undefined)
> While performance generally decreased consistently across adjusters for most classifiers, ComBat-supervised adjustment showed a particularly severe interaction with KNN far worse than its effect on other classifiers. This suggests that KNN's distance-based learning mechanism is particularly sensitive to the specific transformations introduced by supervised batch correction. In contrast, logistic regression showed relative robustness to most adjustment methods, with only ComBat-supervised causing significant degradation.

> [!question]- What is the mechanism of adjuster-classifier interactions? (status:: undefined)
> The observed robustness of logistic regression likely stems from its global linear decision boundary, which is less sensitive to local distributional shifts. In contrast, the high sensitivity of KNN to batch adjustment—particularly supervised methods—highlights the danger of "local" learning. When supervised adjustment shifts samples to satisfy class-based mean/variance constraints, it creates high-density clusters in feature space. KNN "sees" these technical artifacts as biological proximity, leading to a collapse in cross-study generalizability. The severe interaction between ComBat-supervised and KNN arises because supervised adjustment uses class labels during batch correction, potentially creating artificial separation in feature space that KNN's distance-based approach exploits, leading to overfitting. The poor performance of FastMNN across all classifiers suggests that mutual nearest neighbor-based correction, while effective for discrete cell populations in single-cell data, may disrupt continuous biological variation patterns in bulk RNA-seq data.



## Other Warnings

> [!question]- Imbalanced data (status:: undefined)
> Imbalanced data can lead to heavy confounding between the batch and the target variable. When two datasets are adjusted and merged together, adjusters that are unaware of the class imbalance

> [!question]- Using labels, or known groups for batch adjustment (status:: undefined)
> Using class labels or known groups for batch adjustment—supervised adjustment—can lead to overfitting and poor generalization. The adjustment process may inadvertently remove biological signal along with batch effects when class labels are used.

> [!question]- Results (Figure 3) (status:: undefined)
> ![Figure 3: Unbalanced TB Analysis](../figures/class_imbalance_trend.png) *Figure 3: This figure compares ComBat adjustment using target labels to the unsupervised version and to unadjusted data. The y axis shows mean rank comparing the three strategies for each test set and iteration. Vertical bars show the standard error. The results are generally consistent for balanced and imbalanced data (circles and triangles).*

> [!question]- Show something about imbalanced training data (status:: undefined)
> In imbalanced situations, where the supervised adjustment does control for the confounding effect of the imbalance. However, this effect is only seen in the most pronounced imbalance scenarios.

> [!question]- Within Study vs Cross Validation (Figure 4) (status:: undefined)
> ![Figure 4: Within Study vs Cross Validation](../figures/) *Figure 4: Within Study vs Cross Validation* Figure 4 reveals that most batch adjustment methods result in negative deltas (performance decreases) compared to within-study baseline, indicating that cross-study generalization is inherently more challenging than within-study validation.



## The Limitations of Internal Cross-Validation

> [!question]- Why comparing to internal cross validation performance may be misleading (status:: undefined)
> Comparing internal cross-validation performance to cross-study performance reveals important limitations of standard evaluation approaches. Cross-validation within a single study may give optimistic performance estimates because the classifier can learn batch-specific patterns that do not generalize. This finding emphasizes the importance of independent validation cohorts for obtaining realistic performance estimates. [source:: C_03]



## Batch Adjustment Versus Meta-Analysis

> [!question]- When should you merge datasets versus perform meta-analysis? (status:: undefined)
> The choice between batch adjustment (merging datasets) and meta-analysis represents a fundamental decision in multi-study integration. While batch adjustment allows direct combination of datasets into a single pooled analysis, meta-analysis approaches combine statistical results across studies without merging the raw data. [source:: C_16]

> [!question]- What are the trade-offs? (status:: undefined)
> Merging provides greater statistical power for gene discovery when batch effects can be adequately corrected. However, meta-analysis has advantages when: - Raw data are not available or cannot be shared due to privacy constraints - Studies use fundamentally different platforms or technologies that are difficult to harmonize - Batch effects are so severe that correction would remove biological signal - The goal is to identify only the most robust, consistently replicated findings across studies **For classifier development specifically:** Merging with batch correction is generally preferred because classifiers require access to individual sample-level data to learn decision boundaries. Meta-analysis of classifier performance across studies can complement this by assessing consistency, but cannot replace the need for integrated training data. The choice depends on data availability, the severity of batch effects, sample sizes, and analytical goals (differential expression vs pathway analysis vs biomarker discovery). [source:: C_16, C_16]



## The Hierarchy of Concerns

> [!question]- What to do? (status:: undefined)
> The results suggest a hierarchy of concerns for practitioners working with multi-study gene expression data. The most impactful decision is classifier architecture: choosing methods with built-in regularization has the largest effect on performance. Next in importance is validation strategy, where cross-study validation rather than within-study cross-validation determines whether performance estimates are realistic. The choice of batch correction method matters, but less than classifier choice; unsupervised methods appropriate for the data type should be preferred. Supervised correction using class labels during batch adjustment should be avoided entirely, as it risks catastrophic failure modes. Finally, generalization should be evaluated by testing on truly independent cohorts with different technical characteristics to validate real-world applicability.



## Generalizability to Other Omics

> [!question]- Highlight distribution differences across modalities (status:: undefined)
> The specific statistical properties differ across modalities, requiring modality-specific adaptations of batch correction methods, yet the underlying principles remain constant.

> [!question]- RNA-seq: Negative Binomial distribution (status:: undefined)
> RNA-seq count data follow a negative binomial distribution, characterized by over-dispersion where the variance exceeds the mean. This distributional property motivates methods like ComBat-Seq that explicitly model the negative binomial structure rather than assuming Gaussian distributions. The integer nature of counts and the mean-variance relationship are fundamental characteristics that batch correction methods must respect. [source:: C_07]

> [!question]- DNA Methylation: Beta distribution (use limma or ComBat on M-values) (status:: undefined)
> DNA methylation beta values are bounded between 0 and 1, representing the proportion of methylated sites, and approximately follow a beta distribution. This bounded nature violates the assumptions of methods designed for unbounded continuous or count data (Du 2010). Practitioners typically transform beta values to M-values (log2 ratio of methylated to unmethylated intensities) before applying ComBat or limma, as M-values have better statistical properties for differential methylation analysis (Du 2010). The recommended practice is to use M-values for statistical testing while reporting beta-values for biological interpretation, balancing statistical validity with interpretability (Du 2010). Specialized methods like GMQN (Gaussian Mixture Quantile Normalization) have been developed specifically for methylation data. [source:: C_93]

> [!question]- Protein abundance (Mass Spec): Often log-normal (status:: undefined)
> Mass spectrometry data for protein abundance are often log-normally distributed and may have substantial missingness due to detection limits. These data remain inherently biased due to sample handling and instrumentation differences (Välikangas 2016). ComBat's Gaussian assumptions may be more appropriate for log-transformed proteomics data than for RNA-seq counts, though the high degree of missingness requires specialized handling that differs from RNA-seq workflows (Välikangas 2016). [source:: C_96]

> [!question]- Key insight: While the math changes, the strategy of location/scale adjustment is foundational (status:: undefined)
> Despite these distributional differences, the fundamental strategy of location/scale adjustment remains constant across modalities. Whether adjusting the mean and variance of Gaussian-distributed M-values, the dispersion parameters of negative binomial counts, or the ionization efficiency of log-normal protein abundances, the core principle is the same: estimate systematic technical variation and remove it while preserving biological signal. The math changes—negative binomial models for RNA-seq, beta/M-value transformations for methylation, log-normal models for proteomics—but the conceptual framework of identifying and removing batch-specific location and scale parameters is foundational to understanding batch correction across all high-throughput omics technologies.

> [!question]- What core principles apply universally across omics? (status:: undefined)
> The core principles discussed in this chapter apply universally across omics modalities. Systematic technical variation exists in all high-throughput data, and the principles of location and scale adjustment remain constant even as distributional assumptions change. The need to preserve biological signal while removing technical noise is universal, as is the importance of validating through cross-study performance regardless of data type. Avoiding supervised correction prevents catastrophic overfitting across all modalities.

