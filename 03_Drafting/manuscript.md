# Impacts of batch effects on the performance of machine learning classifiers across multiple studies

#### In "Artificial Intelligence and Machine Learning in Genomics and Precision Medicine"


## Abstract

**What is the opening framing for the chapter?** <!-- [ANSWERED] --> The integration of genomic datasets is a cornerstone of modern precision medicine, necessitated by the need for increased statistical power and the validation of biological patterns across diverse populations. <!-- Source: C_99 (Collins2015), C_100 (Ginsburg2018) --> By aggregating data from independent cohorts, researchers can move beyond the limitations of small sample sizes to build more robust machine learning classifiers. <!-- Source: C_99 (Collins2015), C_100 (Ginsburg2018) -->

**When is it useful to combine datasets?** <!-- [RESEARCH NEEDED] --> Combining datasets becomes useful when individual studies have limited sample sizes, when validation across independent cohorts is needed, or when meta-analyses can reveal consistent patterns across diverse populations. <!-- Source: TASK: Need second source, C_03 (Soneson2014) --> Larger sample sizes increase statistical power to detect biological phenomena, enable more robust classifier training, and improve generalizability of findings. <!-- Source: C_03 (Soneson2014) -->

**What is the central problem?** <!-- [ANSWERED] --> This potential is frequently undermined by "batch effects"—systematic technical variations that can lead a classifier to learn how to distinguish between experimental batches rather than meaningful biological conditions. <!-- Source: C_03 (Soneson2014) --> Batch effects can substantially degrade classifier performance when applied to new batches, and even when batch labels are known, they can inflate performance estimates obtained through cross-validation within a single study while simultaneously causing poor performance on independent datasets. <!-- Source: C_03 (Soneson2014) -->

**What is the chapter's scope?** <!-- [ANSWERED] --> This chapter examines how batch effects degrade predictor performance and explores statistical adjustment methods designed to mitigate these artifacts while preserving biological signal. <!-- Source: Summarization of chapter -->

---

## Background and Significance

### Classification in Precision Medicine

**What is the opening topic sentence for classification?** <!-- [RESEARCH NEEDED] --> Classification—the task of assigning observations to predefined categories based on their features—has become essential for biomedical research and clinical practice. <!-- TASK: Is this even true? Is is exaggerated? Has classification reached the clinic? -->

**What is classification?** <!-- [ANSWERED] --> Classification is the task of assigning observations to predefined categories based on their features. <!-- Source: General knowledge -->

**Why is classification useful for biomedical tasks?** <!-- [ANSWERED] --> Machine learning classifiers have demonstrated strong performance for cancer classification tasks using gene expression data. <!-- Source: C_05 (Alharbi2023) --> In precision medicine, molecular profiling enables tailored treatment decisions, with multi-gene prognostic signatures guiding therapeutic interventions for individual patients. <!-- Source: C_99 (Collins2015), C_100 (Ginsburg2018) --> For example, multi-gene prognostic signatures including Oncotype DX, EndoPredict, and Prosigna are widely used clinically to predict recurrence risk in ER+ breast cancer and guide chemotherapy decisions. <!-- Source: C_105 (Buus2021), C_106 (Buus2021) -->

**What data is used for biomedical classification?** <!-- [ANSWERED] --> Biomedical classification tasks utilize diverse data types, including gene expression profiles, protein abundance measurements, genomic sequences, clinical variables, and imaging data. <!-- Source: General knowledge, C_05 (Alharbi2023) --> Each of these data modalities provides complementary information about biological systems and disease mechanisms. <!-- Source: General knowledge -->

### The Clinical Stakes

**Why does batch effect mitigation matter clinically?** <!-- [ANSWERED] --> In precision medicine, even modest drops in classifier performance due to batch effects can mean the difference between accurate diagnosis and misclassification. <!-- Source: General knowledge --> Multi-gene prognostic signatures including Oncotype DX, EndoPredict, and Prosigna are widely used clinically to estimate recurrence risk and guide treatment decisions in breast cancer patients, guiding which patients receive chemotherapy in addition to endocrine therapy. <!-- Source: C_105 (Buus2021), C_106 (Buus2021), C_107 (Buus2021) --> If batch effects go undetected, predictors developed for clinical outcomes may produce results that are more variable than expected, with batch effects having strong adverse effects on predictors and resulting in lower-than-expected classification rates that might put patients at risk. <!-- Source: C_108 (Leek2010) --> Single-patient data processing is vital to the translation of molecular assays, as patient samples in clinical settings are typically collected in small numbers, often one at a time, making batch effect correction essential for clinical translation. <!-- Source: C_109 (Talhouk2016) -->

### The Imperative to Combine Datasets

**When is it useful/necessary to combine datasets?** <!-- [ANSWERED] --> Combining datasets is useful when logistical considerations restrict sample size, when sequential data collection is required, or when validation across independent cohorts is needed. <!-- Source: C_03 (Soneson2014), C_01 (Johnson2007) --> Larger sample sizes increase statistical power, enable validation across independent cohorts, and improve the generalizability of findings. <!-- Source: TASK: is this always true? Hedge or add nuance. -->

**How should GEO be introduced?** <!-- [ANSWERED] -->  <!-- Source: General knowledge --> With over 200,000 studies and 6.5 million samples, GEO facilitates the generation of consistently computed RNA-seq count matrices. <!-- Source: C_31 (Clough2023), C_32 (Clough2023) TASK: Fix citations. TASK: Reduce repetition, this is said also below.-->

**What is the key insight about data reuse?** <!-- [ANSWERED] --> This wealth of data is widely reused for diverse applications, including identifying novel gene expression patterns, finding disease predictors, and developing computational methods. <!-- Source: C_35 (Clough2023) --> Yet the utility of this data is contingent upon our ability to mitigate technical noise. <!-- Source: General knowledge -->

### The Challenge of Batch Effects

**Where do batch effects come from?** <!-- [ANSWERED] --> The source of batch effects varies by data type, but there are common patterns. <!-- Source: Summarization of following sentences --> 

**How do batch effects sources vary by data type? **<!-- [RESEARCH NEEDED] --> Batch effects arise from differences in experimental protocols, equipment, reagent lots, processing dates, and other technical factors that vary between studies or even within studies over time. TASK: detail specific types of data, explain similarities and differences.<!-- Source: C_01 (Johnson2007), C_02 (Zhang2020) -->

**Are predictors on any of these types of data impacted by batch effects?** <!-- [ANSWERED] --> Yes, batch effects impact classifiers across all these data types, leading to overly optimistic performance estimates when using cross-validation within a single batch, while simultaneously causing poor generalization to independent datasets. <!-- Source: C_03 (Soneson2014) --> 

**How can batch effects can be remedied?** <!-- [ANSWERED] --> Statistical adjustment methods, commonly referred to as adjusters or batch correction methods, aim to remove technical variation while preserving biological signal. <!-- Source: C_01 (Johnson2007), C_02 (Zhang2020) --> The balance between these objectives varies across methods and contexts. <!-- Source: General knowledge -->

### Focus on Gene Expression Data

**What type of data will this chapter focus on?** <!-- [RESEARCH NEEDED] --> For our purposes, and without loss of generality, we will focus on RNA sequencing (RNA-seq) data, which has become the dominant technology for measuring gene expression in modern genomics research. <!-- Source: TASK: need source. TASK: isn't microarray sometimes useful, and in fact used in some of the datasets mentioned in this paper? -->

**How is gene expression data generated and used within biological/biomedical research?** <!-- [ANSWERED] --> Gene expression data are generated through high-throughput sequencing technologies that quantify the abundance of RNA transcripts in biological samples. <!-- Source: C_02 (Zhang2020) --> These measurements provide a snapshot of cellular activity, reflecting which genes are actively transcribed under specific conditions, disease states, or in response to treatments. <!-- Source: General knowledge --> Within biological and biomedical research, gene expression data enable the identification of disease-associated pathways, the discovery of therapeutic targets, and the development of diagnostic and prognostic biomarkers. <!-- Source: General knowledge -->

**When is classification for gene expression useful in the context of precision medicine?** <!-- [ANSWERED] --> The application of machine learning methods to gene expression data has shown strong performance for cancer classification tasks. <!-- Source: C_05 (Alharbi2023) --> Clinical gene expression classifiers are already used to guide treatment decisions, with tests like Oncotype DX, EndoPredict, and Prosigna estimating recurrence risk and determining which breast cancer patients should receive chemotherapy in addition to endocrine therapy. <!-- Source: C_105 (Buus2021), C_106 (Buus2021) -->

**Define RNA-seq and GEO ONCE** <!-- [ANSWERED] --> The Gene Expression Omnibus (GEO) serves as a critical international public repository for such data, archiving vast collections of gene expression and epigenomics data from both next-generation sequencing and microarray technologies. <!-- Source: C_31 (Clough2023) --> With over 200,000 studies and 6.5 million samples, GEO facilitates the generation of consistently computed RNA-seq count matrices and offers web-based tools like GEO2R for differential gene expression analysis. <!-- Source: C_31 (Clough2023), C_32 (Clough2023) --> The repository has seen a significant shift towards next-generation sequencing, with NGS comprising 85% of submissions and RNA-seq representing over half of all studies since 2018, while single-cell RNA-seq studies have grown to 21% of all RNA-seq studies by 2022. <!-- Source: C_33 (Clough2023), C_33b (Clough2023), C_34 (Clough2023) --> This wealth of data is widely reused for diverse applications, including identifying novel gene expression patterns, finding disease predictors, and developing computational methods. <!-- Source: C_35 (Clough2023) -->

**What is the key framing about gene expression and batch effects?** <!-- [ANSWERED] --> Gene expression data provide an excellent context for understanding how batch effects can be modeled and removed, as the technical variation introduced by sequencing technologies is well-characterized and substantial. <!-- Source: C_07 (Zhang2020) TASK: Fix citations-->

**How should dataset integration be revisited for gene expression?** <!-- [ANSWERED] --> Integrating gene expression data across multiple studies offers substantial benefits: increased sample sizes improve the robustness of classifiers, independent validation cohorts provide evidence of generalizability, and meta-analyses can reveal consistent patterns across diverse populations. <!-- Source: General knowledge --> However, batch effects pose particular challenges for gene expression data due to the sensitivity of sequencing technologies to technical variation. <!-- Source: C_02 (Zhang2020) -->

**What is the key insight about batch effect magnitude?** <!-- [ANSWERED] --> These technical artifacts can be substantial—often comparable to or larger than the biological signals of interest—making them a primary concern when combining datasets. <!-- Source: C_07 (Zhang2020) -->

**What specific effects do batches have on gene expression?** <!-- [ANSWERED] --> Batch effects manifest in gene expression data as systematic shifts in expression levels between batches, differences in variance structure, and alterations in the relationships between genes. <!-- Source: C_07 (Zhang2020) --> For RNA-seq count data specifically, the data are typically skewed and over-dispersed, which complicates batch correction methods that assume Gaussian distributions. <!-- Source: C_07 (Zhang2020) --> This phenomenon, where batch effects confound performance estimates, represents a critical challenge in developing generalizable classifiers. <!-- Source: C_03 (Soneson2014) -->

### Unknown Batch Effects

**How should unknown batch effects be positioned?** <!-- [ANSWERED] --> Briefly acknowledge unknown batch effects and SVA, but clearly scope them as beyond the chapter's focus on known batch correction. <!-- Source: Outline -->

**First mention of SVA: Define the problem** <!-- [ANSWERED] --> In some cases, batch labels may be unknown or only partially known, particularly when working with publicly available data where experimental metadata may be incomplete. <!-- Source: Outline --> Sometimes we don't know the batch labels—especially with public data where complete experimental metadata may not be provided. <!-- Source: Outline --> Methods such as surrogate variable analysis (SVA) can identify and adjust for unknown batch effects by extracting surrogate variables from high-dimensional data that capture unwanted effects. <!-- Source: C_11 (Leek2007) --> SVA was introduced by Leek and Storey (2007) to model unknown, latent sources of variation in genomics data, and is available in the Bioconductor sva package. <!-- Source: C_11 (Leek2007) --> This topic extends beyond the scope of the present chapter, which focuses on supervised batch correction methods where batch labels are known. <!-- Source: Outline --> A second discussion of SVA appears in "The Horizon" section, where we examine how SVA is being integrated into modern automated pipelines for advanced applications. <!-- Source: Outline -->

**Transition to classifiers** <!-- [ANSWERED] --> We now transition to examining the classifiers themselves, followed by the adjustment methods designed to address batch effects, and finally the interaction between these two components in determining overall classification performance.

---

## Machine Learning Classifiers for Gene Expression Data

**How should the classifiers section be framed?** <!-- [ANSWERED] --> Frame classifiers in the context of the $p \gg n$ problem (features vastly outnumber samples) that is characteristic of gene expression data. <!-- Source: Outline --> Emphasize the shift from traditional methods to modern approaches. <!-- Source: Outline -->

**What is the opening framing for classifiers?** <!-- [ANSWERED] --> The landscape of gene expression classification has shifted from traditional linear discriminant analysis toward methods capable of handling the $p \gg n$ problem, where the number of features (genes) vastly outnumbers the number of samples. <!-- Source: General knowledge -->

**Explain great performing classifiers in general** <!-- [ANSWERED] --> Machine learning classifiers have demonstrated strong performance across diverse biomedical classification tasks. <!-- Source: C_05 (Alharbi2023) --> The field encompasses both traditional statistical methods and modern machine learning approaches, with the direction of the field increasingly favoring flexible, data-driven methods that can capture complex patterns in high-dimensional data. <!-- Source: C_05 (Alharbi2023) -->

**ML, traditional, broad strokes, direction of field in usage** <!-- [ANSWERED] --> The field has evolved from traditional statistical methods (logistic regression, linear discriminant analysis) toward modern machine learning approaches (support vector machines, random forests, neural networks). <!-- Source: C_05 (Alharbi2023) --> The direction increasingly favors flexible, data-driven methods that can capture complex, non-linear patterns in high-dimensional genomic data. <!-- Source: C_05 (Alharbi2023) -->

### Classifier Architectures

**How should classifiers be organized?** <!-- [ANSWERED] --> Group classifiers into three architectural categories: Regularized Linear Models, Ensemble Methods, and Non-linear Geometric Models. <!-- Source: Outline --> This organization emphasizes how different approaches handle high-dimensional genomic data. <!-- Source: Outline -->

**What specific classifiers do well with gene expression?** <!-- [ANSWERED] --> For gene expression data specifically, several classifier types have shown particular utility: support vector machines (SVM), random forests, logistic regression with regularization, and, when sufficient data are available, neural networks. <!-- Source: C_06 (Alharbi2023) --> Each classifier type has distinct characteristics that make it suitable for different scenarios and data types. <!-- Source: C_06 (Alharbi2023) -->

#### Regularized Linear Models

**What is the key insight about regularization?** <!-- [ANSWERED] --> Built-in regularization is vital for high-dimensional gene expression data; without it, simpler models like standard logistic regression succumb to technical noise in batch-affected datasets. <!-- Source: Results data -->

**How should elastic net be described for genomics?** <!-- [ANSWERED] --> Elastic net is particularly well-suited to the $p \gg n$ problem characteristic of gene expression data, where thousands of genes (features) vastly outnumber samples. <!-- Source: C_79 (Zou2005), C_80 (Zou2005) --> By combining L1 and L2 regularization, elastic net simultaneously performs feature selection (identifying which genes matter) and shrinkage (preventing overfitting to noise). <!-- Source: C_79 (Zou2005), C_80 (Zou2005) --> The L1 penalty drives coefficients of irrelevant genes to exactly zero, creating sparse models that are interpretable and computationally efficient. <!-- Source: C_79 (Zou2005), C_80 (Zou2005) --> The L2 penalty encourages a grouping effect where correlated genes—common in biological pathways—tend to be selected or excluded together, preserving biological coherence. <!-- Source: C_79 (Zou2005), C_80 (Zou2005) --> This dual regularization is particularly effective at ignoring technical noise from batch effects while retaining biological signal, as demonstrated by elastic net's superior performance across adjustment methods in our results. <!-- Source: Results data --> \cite{TXKAG4BA}

**Explain each type of classifier** <!-- [ANSWERED] --> 

Logistic regression without regularization struggles in the $p \gg n$ regime because it attempts to fit a coefficient for every gene, leading to overfitting and instability. <!-- Source: General knowledge --> When combined with regularization techniques such as L1 (lasso) or L2 (ridge) penalties, logistic regression becomes viable for gene expression data by constraining the solution space and preventing overfitting. <!-- Source: C_76 (Zou2005) --> The L1 penalty (lasso) performs automatic feature selection by driving coefficients to exactly zero, while the L2 penalty (ridge) shrinks coefficients toward zero without eliminating them. <!-- Source: C_76 (Zou2005) -->

#### Ensemble Methods

**What is the key characteristic of ensemble methods for genomics?** <!-- [ANSWERED] --> Ensemble methods are robust to the high-dimensional, noisy nature of gene expression data because they aggregate predictions across multiple models, each trained on different subsets of samples and features. <!-- Source: C_84 (Breiman2001) --> This averaging effect reduces sensitivity to outliers and technical artifacts, including batch effects. <!-- Source: C_84 (Breiman2001) -->

Random forests construct ensembles of decision trees, where each tree is trained on a bootstrap sample of the data and a random subset of features. <!-- Source: C_83 (Breiman2001) --> For gene expression data, this means each tree sees a different combination of genes and samples, preventing any single technical artifact from dominating the model. <!-- Source: General knowledge --> The final prediction aggregates votes across all trees, providing robustness to noise and the ability to capture complex interactions between genes. <!-- Source: C_84 (Breiman2001) --> Random forests also provide measures of feature importance, which can aid in biological interpretation by identifying which genes contribute most to classification decisions. <!-- Source: C_75 (DiazUriarte2006) --> \cite{6U3RSD58}

**How should XGBoost be described?** <!-- [ANSWERED] --> XGBoost, a gradient boosting implementation, builds trees sequentially to correct errors from previous iterations, often achieving excellent performance on structured data. <!-- Source: C_81 (Chen2016) --> The method uses a sparsity-aware algorithm for sparse data and provides efficient handling of large-scale datasets through optimized cache access patterns and data compression. <!-- Source: C_81 (Chen2016), C_82 (Chen2016) --> \cite{HC57XGAZ}

#### Non-linear Geometric Models

**What distinguishes geometric models in the genomic context?** <!-- [ANSWERED] --> These models identify decision boundaries in high-dimensional gene expression space using geometric principles. <!-- Source: General knowledge --> For gene expression data, where biological classes may not be linearly separable due to complex regulatory networks and pathway interactions, the ability to learn non-linear boundaries is crucial. <!-- Source: General knowledge -->

Support vector machines identify optimal decision boundaries by finding hyperplanes that maximize the margin between classes. <!-- Source: C_86 (Guyon2002) --> SVMs are particularly effective for the $p \gg n$ problem in gene expression data because they focus on support vectors—the most informative samples near the decision boundary—rather than attempting to model all samples equally. <!-- Source: C_86 (Guyon2002) --> The kernel trick allows SVMs to implicitly map gene expression profiles into higher-dimensional spaces where complex, non-linear biological patterns become linearly separable, making them versatile for diverse biological patterns. <!-- Source: C_86 (Guyon2002) --> \cite{IWRVSPHQ}

Neural networks, including multi-layer perceptrons and more sophisticated architectures, can achieve excellent performance when sufficient data are available. <!-- Source: C_87 (Hanczar2022), C_88 (Hanczar2022) --> For gene expression data, neural networks can learn hierarchical representations where early layers capture individual gene patterns and deeper layers integrate these into pathway-level or systems-level features. <!-- Source: General knowledge --> Neural networks outperform other methods only when training set sizes are very large, as the high-dimensional nature of gene expression data (many features, few samples) has historically limited deep learning effectiveness. <!-- Source: C_87 (Hanczar2022), C_88 (Hanczar2022) --> Deep learning approaches have shown particular promise for identifying complex, non-linear patterns in gene expression data that may be missed by simpler methods. <!-- Source: C_05 (Alharbi2023) --> \cite{GNFV76AH}

**Very general way, the kinds of genomics data that they are useful for** <!-- [ANSWERED] --> These classifiers are useful across various types of genomics data beyond gene expression, including DNA methylation profiles, copy number variations, and protein expression data, though the specific characteristics of each data type may favor certain classifier types. <!-- Source: General knowledge -->

**https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009926** <!-- [ANSWERED] --> Piccolo et al. (2022) demonstrated that classification performance for gene-expression data depends strongly on algorithm choice and performance metric. <!-- Source: C_17 (Piccolo2022) --> The number of samples and genes did not strongly correlate with classification performance, and hyperparameter tuning substantially affects performance. <!-- Source: C_17 (Piccolo2022) --> This finding emphasizes the importance of careful algorithm selection and evaluation metric choice when building predictive models based on gene expression. <!-- Source: C_17 (Piccolo2022) -->

### The Role of Data Scale

**How should data scale be discussed?** <!-- [ANSWERED] --> Transition from classifier architectures to the role of data scale, particularly for neural networks. <!-- Source: Outline -->

**Neural Net if you have enough data, which sometimes happens (some kinds of genomics data)** <!-- [ANSWERED] --> Neural networks can achieve excellent performance when sufficient data are available, though they outperform other methods only when training set sizes are very large. <!-- Source: C_87 (Hanczar2022), C_88 (Hanczar2022) --> For genomics data, this requirement is sometimes met, for example, when combining all available data from public repositories like the Gene Expression Omnibus (GEO), which contains millions of samples across thousands of studies. <!-- Source: C_31 (Clough2023) --> The availability of consistently computed RNA-seq count matrices from resources like GEO facilitates the application of deep learning approaches that can identify complex, non-linear patterns in large-scale gene expression data. <!-- Source: C_32 (Clough2023) --> \cite{GNFV76AH}

**Such as all of Geo (Jeff Leeks) or > 1000** <!-- [ANSWERED] --> This is addressed by the previous point. <!-- Source: C_31 (Clough2023), C_32 (Clough2023) -->


---

## Batch Correction Methods

**How should the adjusters section be framed?** <!-- [ANSWERED] --> Frame batch correction as a principled approach to removing technical variation while preserving biological signal, with gene expression providing an excellent exemplar system. <!-- Source: Outline -->

**Gene expression gives us an excellent insight into how batch effects can be modeled and removed.** <!-- [ANSWERED] --> Gene expression data provide an excellent context for understanding how batch effects can be modeled and removed, as the technical variation introduced by sequencing technologies is well-characterized and substantial. <!-- Source: C_07 (Zhang2020) --> The sensitivity of sequencing technologies to technical variation makes batch effects particularly pronounced in gene expression data. <!-- Source: C_07 (Zhang2020) -->

### A Taxonomy of Batch Correction Methods

**How can batch correction methods be categorized?** <!-- [ANSWERED] --> Batch correction methods can be organized into three main categories based on their underlying approach: Scale/Location methods, Matrix Factorization methods, and Nearest Neighbor methods. <!-- Source: General knowledge -->

**Scale/Location Methods** adjust the mean (location) and variance (scale) of features across batches, assuming batch effects manifest as systematic shifts in these parameters. <!-- Source: General knowledge --> ComBat and its variants (ComBat-Seq, ComBat with mean-only adjustment) exemplify this approach, using Empirical Bayes methods to estimate and remove batch-specific location and scale parameters. <!-- Source: C_01 (Johnson2007), C_02 (Zhang2020) --> These methods work well when batch effects primarily affect the first two moments of the distribution. <!-- Source: General knowledge -->

**Matrix Factorization Methods** decompose the data into biological and technical components, attempting to separate signal from noise through dimensionality reduction. <!-- Source: General knowledge --> Methods like LIGER use non-negative matrix factorization to identify shared biological factors while isolating dataset-specific technical factors. <!-- Source: C_13 (Welch2019) --> Surrogate Variable Analysis (SVA) also falls into this category, extracting latent variables that capture unwanted variation. <!-- Source: C_11 (Leek2007) --> These methods are particularly useful when batch effects are complex and cannot be adequately modeled by simple location/scale adjustments. <!-- Source: General knowledge -->

**Nearest Neighbor Methods** identify corresponding samples or features across batches and use these correspondences to align datasets. <!-- Source: General knowledge --> MNN and FastMNN exemplify this approach, finding mutual nearest neighbors between batches and using these anchors to correct batch effects. <!-- Source: C_14 (Stuart2019) --> While effective for single-cell data with discrete cell populations, these methods struggle with the continuous variation characteristic of bulk RNA-seq data, as demonstrated by our results. <!-- Source: Results data -->

### The ComBat Framework

**The ComBat model works well for bulk RNA.** <!-- [ANSWERED] --> The ComBat model works particularly well for bulk RNA sequencing data, where it has become a standard method for batch correction. <!-- Source: C_01 (Johnson2007), C_08 (Johnson2007) --> ComBat uses Empirical Bayes methods to estimate location (L) and scale (S) parameters for each batch, assuming the data follow a Gaussian distribution. <!-- Source: C_01 (Johnson2007) --> This approach allows ComBat to borrow information across genes when estimating batch effects, making it robust even when the number of samples per batch is small. <!-- Source: C_01 (Johnson2007) -->

**Where else does ComBat work well?** <!-- [ANSWERED] --> ComBat was originally developed for microarray data and has been successfully applied to bulk RNA-seq data. <!-- Source: C_08 (Johnson2007) --> While standard ComBat assumes a Gaussian distribution—a condition often violated by the raw, over-dispersed nature of RNA-seq countsactitioners frequently apply it to log-transformed data to approximate normality. <!-- Source: C_07, 2 --> However, this transformation can stabilize variance at the cost of distorting the underlying count structure. <!-- Source: 2 --> \cite{SY5YRHHX} ComBat-Seq offers a superior alternative by modeling the data directly via a negative binomial distribution, thus preserving the integer nature of the counts and providing a more principled approach for modern sequencing pipelines. <!-- Source: C_02, 2 -->

**Why learn about ComBat?** <!-- [ANSWERED] --> Beyond its effectiveness for bulk RNA data, ComBat exemplifies the Empirical Bayes approach to batch correction, which has influenced the development of many subsequent methods. <!-- Source: C_01, C_10, 1, 6 TASK: Fix citations --> Understanding ComBat provides insight into the general principles of batch correction: identifying systematic technical variation, estimating its magnitude, and removing it while preserving biological signal. <!-- Source: C_01, 1 TASK: Fix citations -->

### Methods for Other Modalities

**In other modalities, what other techniques are used?** <!-- [ANSWERED] --> For single-cell RNA sequencing data, methods such as Harmony, LIGER, and Seurat have been developed to address the unique challenges of single-cell data, including sparsity, high dimensionality, and the need to preserve cell type identity. <!-- Source: C_09 (Tran2020), C_12 (Korsunsky2019), C_13 (Welch2019), C_14 (Stuart2019) --> Harmony integrates single-cell datasets by removing batch effects while preserving biological structure through iterative batch-centroid correction in PC space, and is fast and scalable. <!-- Source: C_12 (Korsunsky2019) --> LIGER uses integrative non-negative matrix factorization (iNMF) to separate shared biological factors from dataset-specific technical factors, performing well when batches have non-identical cell type compositions. <!-- Source: C_13 (Welch2019) --> Seurat v3 uses anchor-based integration with mutual nearest neighbors (MNNs) to correct batch effects while preserving cell-type structure. <!-- Source: C_14 (Stuart2019) --> However, these methods were developed specifically for single-cell RNA sequencing data and are not appropriate for bulk RNA-seq data, where samples represent continuous mixtures of cell types rather than discrete cell populations. <!-- Source: C_09 (Tran2020), Results data -->

### The Failure of Mutual Nearest Neighbors for Bulk RNA-Seq

**We will explore the use of MNN for bulk RNA, and show it doesn't work well.** <!-- [ANSWERED] --> The analysis included both MNN (mutual nearest neighbors) and FastMNN as batch correction methods for bulk RNA-seq data. <!-- Source: Results data --> FastMNN showed consistently poor performance across all classifiers, with mean MCC differences from baseline ranging from -0.148 (XGBoost) to -0.441 (shrinkage LDA), all highly significant ($p < 10^{-5}$). <!-- Source: adjusters_on_classifiers_relative_aggregated_significance.csv --> Standard MNN also showed significant performance decreases, though less severe than FastMNN, with mean differences ranging from -0.114 (KNN) to -0.281 (shrinkage LDA). <!-- Source: adjusters_on_classifiers_relative_aggregated_significance.csv --> These results suggest that mutual nearest neighbor-based correction methods, while effective for discrete cell populations in single-cell data, may disrupt continuous biological variation patterns in bulk RNA-seq data. <!-- Source: Results data -->

**Why does MNN fail for bulk RNA-seq? The "Islands vs. Continent" Problem** <!-- [ANSWERED] --> MNN was designed for single-cell data where discrete cell populations exist as "islands"—distinct cell types with clear boundaries in expression space. <!-- Source: 10 TASK: Fix citation --> \cite{7PZXDBIM} The method works by finding mutual nearest neighbors between these islands across batches, assuming that the same cell types exist in both datasets. <!-- Source: 10 TASK: Fix citation --> \cite{7PZXDBIM} Bulk RNA-seq data, in contrast, represent a "continent"—continuous variation across samples with no discrete boundaries. Each bulk sample is an aggregate of multiple cell types, creating a smooth gradient of expression rather than distinct clusters. When MNN attempts to find "nearest neighbors" in this continuous space, it creates artificial discontinuities by forcing samples into discrete correspondence, disrupting the biological gradients that classifiers need to learn. This fundamental mismatch between the method's assumptions (discrete populations) and the data structure (continuous variation) explains why MNN-based approaches consistently degrade classifier performance on bulk RNA-seq data. <!-- Source: Results data TASK: Need published source to support this "islands vs continent" explanation -->

### The Link Between Adjustment and Classifier Performance



**We will explore the use of MNN for bulk RNA, and show it doesn't work well.** <!-- [ANSWERED] --> The analysis included both MNN (mutual nearest neighbors) and FastMNN as batch correction methods for bulk RNA-seq data. <!-- Source: Results data --> FastMNN showed consistently poor performance across all classifiers, with mean MCC differences from baseline ranging from -0.148 (XGBoost) to -0.441 (shrinkage LDA), all highly significant ($p < 10^{-5}$). <!-- Source: adjusters_on_classifiers_relative_aggregated_significance.csv --> Standard MNN also showed significant performance decreases, though less severe than FastMNN, with mean differences ranging from -0.114 (KNN) to -0.281 (shrinkage LDA). <!-- Source: adjusters_on_classifiers_relative_aggregated_significance.csv --> These results suggest that mutual nearest neighbor-based correction methods, while effective for discrete cell populations in single-cell data, may disrupt continuous biological variation patterns in bulk RNA-seq data. <!-- Source: Results data TASK: Need published source to support this interpretation -->

**Why does classifier performance depend on adjustment?** <!-- [ANSWERED] --> Batch effects can introduce systematic biases that classifiers learn to exploit, leading to inflated performance on training data but poor generalization. <!-- Source: 4CFFLXQX --> Effective batch correction removes these technical artifacts, allowing classifiers to focus on biological signals and improving cross-study performance. <!-- Source: General knowledge -->

**Why is cross study performance a good indicator of both biological preservation and batch reduction?** <!-- [ANSWERED] --> Cross-study performance serves as a good indicator of both biological preservation and batch reduction because it directly tests whether a classifier can generalize to independent datasets with potentially different technical characteristics. <!-- Source: 4CFFLXQX --> High cross-study performance suggests that batch effects have been successfully removed while biological signal has been preserved. <!-- Source: General knowledge TASK: Add detail to this explanation; enhance understanding of why this is true. Source should be logic. -->

**What are some common evaluation metrics, and why are they not as good? (Limitations)** <!-- [ANSWERED] --> Common evaluation metrics such as within-study cross-validation can be misleading in the presence of batch effects. <!-- Source: 4CFFLXQX  --> Cross-validation within a single study may give optimistic performance estimates because the classifier can learn batch-specific patterns that do not generalize. <!-- Source: 4CFFLXQX  --> These limitations highlight the importance of using cross-study validation to obtain realistic performance estimates. <!-- Source: 4CFFLXQX  -->

**PCA** <!-- [ANSWERED] --> Principal component analysis (PCA) and other visualization methods can help identify batch effects but do not directly measure their impact on classifier performance. <!-- Source: General knowledge TASK: Need source --> **Add common pitfall:** While PCA can confirm that batches now 'overlap' visually, it cannot guarantee that the biological signal needed for classification has been preserved. Successful batch mixing in a 2D PCA plot is a necessary but insufficient condition for a generalizable classifier. <!-- Source: revision_report.md TASK: Need published source for this important caveat --> BatchQC provides interactive software for evaluating sample and batch effects with PCA, heatmaps, dendrograms, and other diagnostics. <!-- Source: C_15, 11 TASK: Fix citations --> The tool supports multiple batch correction methods including ComBat, ComBat-Seq, limma, and SVA, and includes metrics such as kBET for quantifying batch mixing. <!-- Source: C_15, 11 TASK: Fix citations -->

### Clinical Portability: The Single Sample Problem

**What is the single sample problem in precision medicine?** <!-- [ANSWERED] --> Most batch correction methods require a "batch" of samples to estimate and remove technical variation—they need multiple samples from each batch to calculate batch-specific parameters. <!-- Source: 40, 41 TASK: Fix citations --> \cite{WK56IHGB, 9ARJGGTA} In precision medicine, however, we often face the "batch of one" scenario: a single patient sample arrives at the clinic, processed on whatever equipment is available that day, by whichever technician is on duty. <!-- Source: 40, 41 TASK: Fix citations --> \cite{WK56IHGB, 9ARJGGTA} How do we apply a classifier trained on batch-corrected research data to this new, single sample that constitutes its own unique "batch"? <!-- Source: 40, 41 TASK: Fix citations --> \cite{WK56IHGB, 9ARJGGTA}

**What are the implications for clinical deployment?** <!-- [ANSWERED] --> This single sample problem has profound implications for translating research classifiers into clinical practice. <!-- Source: 40, 41 TASK: Fix citations --> \cite{WK56IHGB, 9ARJGGTA} A classifier trained on ComBat-adjusted data from multiple research cohorts cannot apply ComBat to a single incoming patient sample—there is no batch to correct. <!-- Source: 40, 41 TASK: Fix citations --> The classifier must either be robust to the technical variation of the new sample's processing conditions, or we must develop alternative strategies such as reference-based normalization methods that can adjust a single sample against a pre-defined reference distribution. <!-- Source: 40, 41 TASK: Fix citations --> \cite{WK56IHGB, 9ARJGGTA} This challenge underscores why cross-study validation, which tests generalization to new technical conditions, is a better proxy for clinical performance than within-study cross-validation. <!-- Source: C_04, 3 TASK: Fix citations -->

**What solutions exist for single sample correction?** <!-- [ANSWERED] --> Emerging approaches to address the single sample problem include reference-based normalization, where a single sample is adjusted against a pre-computed reference distribution from training data. <!-- Source: 40, 41 TASK: Fix citations --> \cite{WK56IHGB, 9ARJGGTA} Frozen robust multiarray analysis (fRMA) pioneered this approach for microarrays by precomputing probe-specific effects and variances from large public databases, allowing individual arrays to be normalized without requiring a batch. <!-- Source: 40 TASK: Fix citation --> \cite{WK56IHGB} Similar reference-based strategies have been developed for other platforms, including NanoString nCounter data, demonstrating that single-patient molecular testing is feasible with appropriate normalization methods. <!-- Source: 41 TASK: Fix citation --> \cite{9ARJGGTA} The choice of classifier architecture also matters: methods with strong built-in regularization, like elastic net, may be more robust to technical variation in single samples than methods that rely heavily on local structure, like KNN. <!-- Source: Results data TASK: Need published source to support this claim -->

---

## Case Study: Cross-Study Tuberculosis Classification

**How should the TB results be framed?** <!-- [ANSWERED] --> Rather than presenting this as a standalone experimental section, frame it as a case study that illustrates the concepts discussed earlier—demonstrating how batch effects, adjustment methods, and classifier choice interact in a real-world scenario. <!-- Source: Outline -->

### Datasets: A Natural Stress Test for Batch Correction

**Why were these specific TB datasets chosen to represent "Real World Noise"?** <!-- [ANSWERED] --> To rigorously evaluate the impact of batch effects on classifier performance, we selected tuberculosis gene expression datasets that represent "Real World Noise"—the ultimate test of cross-population generalizability. <!-- Source: General knowledge TASK: Need source --> Rather than choosing datasets that are technically similar, we deliberately selected studies that juxtapose adolescent and adult prospective cohorts with pediatric and adult case-control studies, household contact studies with clinical diagnostic studies, whole blood samples with sputum specimens, and data collected across four continents using different sequencing platforms. <!-- Source: General knowledge TASK: Need source --> This heterogeneity is not a limitation but a feature: if batch correction methods can preserve biological signal while removing technical artifacts across this diversity, they are likely to succeed in real-world precision medicine applications. <!-- Source: General knowledge TASK: Need source -->

**How does the heterogeneity (adolescent/adult, blood/sputum, multiple continents) serve as a feature, not a limitation?** <!-- [ANSWERED] --> The technical heterogeneity—different laboratories, sequencing platforms, RNA extraction protocols, and processing dates—creates the batch effects that batch correction methods must address. <!-- Source: General knowledge TASK: Need source --> The biological heterogeneity—age, HIV status, sample type, and population genetics—represents the diversity that classifiers must generalize across. <!-- Source: General knowledge TASK: Need source --> If a classifier trained on South African adolescent blood samples can accurately predict tuberculosis status in Indian adult blood samples or distinguish HIV-associated differences in Ugandan sputum samples, we have achieved the cross-population generalizability that precision medicine demands. <!-- Source: General knowledge TASK: Need source -->

**What is the common biological thread across datasets that makes them comparable?** <!-- [ANSWERED] --> While most datasets focus on the classification task of distinguishing active tuberculosis from latent infection, the collection also includes studies examining TB progression risk and HIV-associated differences in TB pathogenesis. <!-- Source: C_69 (Zak2016), C_70 (Suliman2018), C_72 (Walter2016) --> This diversity of biological questions, while introducing additional complexity, tests whether batch correction methods can preserve distinct biological signals across different experimental designs and research objectives. <!-- Source: General knowledge TASK: Need source --> The primary classification task—active versus latent TB—remains constant across most datasets, providing a common thread for evaluating classifier generalization, though the Walter et al. study examines HIV status differences within active TB patients rather than active vs latent classification. <!-- Source: C_72 (Walter2016) -->

**What technical details should be captured in a summary table?** <!-- [ANSWERED] --> The table should capture Study, Region, Population, Sample Type, Design, and Key Characteristics to provide a comprehensive view of the heterogeneity. <!-- Source: Outline -->

| Study | Region | Population | Sample Type | Design | Key Characteristics |
|-------|--------|------------|-------------|--------|---------------------|
| Zak et al. (2016) \cite{ATTXF4B6} | South Africa | Adolescents (12-18 years) | Whole blood | Prospective cohort | Longitudinal sampling every 6 months to predict progression <!-- Source: C_69 (Zak2016) --> |
| Suliman et al. (2018) \cite{Suliman2018_Key} | South Africa, Gambia, Ethiopia | Adults | Whole blood | Prospective cohort | Household contacts, RISK4 four-gene signature for TB progression <!-- Source: C_70 (Suliman2018) --> |
| Anderson et al. (2014) \cite{Anderson2014_Key} | South Africa, Malawi, Kenya | Children | Whole blood | Case-control | Childhood TB diagnosis, 51-transcript signature <!-- Source: C_68 (Anderson2014) --> |
| Leong et al. (2018) \cite{AABTK2K9} | India | Adults | Whole blood | Case-control | South Indian population, active vs latent <!-- Source: C_71 (Leong2018) --> |
| Walter et al. (2016) \cite{Walter2016_Key} | Gambia, Uganda | Adults | Sputum | Case-control | HIV+ vs HIV- in active TB patients (different classification task) <!-- Source: C_72 (Walter2016) --> |
| Kaforou et al. (2013) \cite{GVWW6MZB} | South Africa | Adults (Xhosa, 18+) | Whole blood | Case-control | HIV-infected and -uninfected cohorts <!-- Source: C_73 (Kaforou2013) --> |

**How does this collection test cross-population generalizability?** <!-- [ANSWERED] --> This collection spans adolescent and adult prospective cohorts, pediatric and adult case-control studies, whole blood and sputum samples, and four continents (Africa, Asia, North America) with diverse genetic backgrounds and HIV co-infection patterns. <!-- Source: C_68 (Anderson2014), C_69 (Zak2016), C_70 (Suliman2018), C_71 (Leong2018), C_72 (Walter2016), C_73 (Kaforou2013) --> The combination tests whether classifiers can generalize across age groups, sample types, geographic populations, study designs, and even different but related classification tasks—the full spectrum of variation encountered in real-world clinical deployment. <!-- Source: General knowledge TASK: Need source -->

**What makes this a "stress test" rather than a limitation?** <!-- [ANSWERED] --> By deliberately maximizing heterogeneity, we create the most challenging possible scenario for batch correction and classifier generalization. <!-- Source: General knowledge TASK: Need source --> Success in this stress test provides strong evidence that the methods will work in less extreme real-world scenarios. <!-- Source: General knowledge TASK: Need source --> Failure reveals which methods are fragile and which are robust to real-world complexity. <!-- Source: General knowledge TASK: Need source -->

### Classifier Performance Rankings: Lessons Learned

**What is the key finding about classifier hierarchy?** <!-- [ANSWERED] --> Despite the variety of adjustment methods, a clear hierarchy emerges with elastic net and random forests consistently outperforming other methods, demonstrating the importance of built-in regularization for high-dimensional gene expression data. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper -->

**How should Figure 1 be described?** <!-- [ANSWERED] --> Despite the variety of adjustment methods, a clear hierarchy of classifiers emerges in Figure 1. <!-- Source: Outline -->

**Methods** <!-- [ANSWERED] --> The analysis evaluated classifier performance across multiple tuberculosis gene expression datasets using leave-one-study-out cross-validation. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> Nine machine learning classifiers were tested: elastic net (regularized logistic regression), k-nearest neighbors (KNN), logistic regression, neural networks, random forests, shrinkage linear discriminant analysis (LDA), support vector machines (SVM), and XGBoost. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> Ten batch adjustment methods were compared: ComBat, ComBat with mean-only adjustment, ComBat-Seq supervised adjustment, naive merging (unadjusted), mutual nearest neighbors (MNN), FastMNN, nonparanormal transformation (NPN), rank-based normalization applied twice, rank-based normalization of samples, and within-study cross-validation as a baseline. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> Performance was assessed using multiple metrics including Matthews correlation coefficient (MCC), accuracy, balanced accuracy, area under the ROC curve (AUC), sensitivity, and specificity. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> The experimental design included 3 to 6 datasets per analysis configuration, with test studies including GSE37250_SA (South Africa), GSE37250_M (Malawi), GSE39941_M (Malawi), India, USA, and Africa cohorts. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper -->

**Results (Figure 1)** <!-- [ANSWERED] -->
![Figure 1: Average Rank by Classifier](figures/average_rank_by_classifier.png)
*Figure 1: Classifier performance rankings aggregated across all batch adjustment methods. Elastic net and random forests consistently outperform other methods, demonstrating the importance of built-in regularization for high-dimensional gene expression data.*

**What does Figure 1 reveal about classifier rankings?** <!-- [ANSWERED] --> Across all batch adjustment methods, elastic net and random forest classifiers demonstrated the strongest overall performance, followed closely by neural networks, SVM, and XGBoost. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> KNN showed moderate performance, while logistic regression without regularization performed poorly, likely due to the high-dimensional nature of gene expression data. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> These rankings remained relatively stable across different adjustment methods, suggesting that classifier choice has a substantial impact on performance independent of batch correction approach. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper -->

**Why does elastic net's regularization specifically ignore technical noise?** <!-- [ANSWERED] --> Elastic net's superior performance stems from its dual regularization mechanism that is particularly well-suited to distinguishing biological signal from technical noise. <!-- Source: C_79 (Zou2005), C_80 (Zou2005) TASK: Fix citations --> The L1 penalty performs automatic feature selection, driving coefficients of genes that vary primarily due to batch effects toward zero, since these genes will not consistently predict the outcome across different batches. <!-- Source: C_79 (Zou2005) TASK: Fix citations --> The L2 penalty stabilizes the solution by grouping correlated genes, which is crucial because biologically meaningful genes often work in coordinated pathways, while batch-affected genes tend to vary independently. <!-- Source: C_80 (Zou2005) TASK: Fix citations --> This combination means that elastic net naturally "ignores" technical noise—genes whose expression varies due to batch effects fail to receive consistent non-zero weights across cross-validation folds, while genes with genuine biological signal receive stable, non-zero coefficients. <!-- Source: C_79 (Zou2005), C_80 (Zou2005) TASK: Fix citations --> This explains why elastic net maintains strong performance even with imperfect batch correction: its regularization provides an additional layer of protection against technical artifacts. <!-- Source: Results data TASK: Need published source to support this interpretation -->

**What is the mechanism: L1/L2 mechanics?** <!-- [ANSWERED] --> The L1 penalty (lasso) drives coefficients to exactly zero, performing automatic feature selection by eliminating genes that don't consistently predict across batches. <!-- Source: C_79 (Zou2005) TASK: Fix citations --> The L2 penalty (ridge) shrinks coefficients toward zero without eliminating them, and encourages grouping of correlated features—biologically related genes in pathways tend to be selected or excluded together. <!-- Source: C_80 (Zou2005) TASK: Fix citations --> Together, these mechanics create a model that is sparse (few genes), stable (correlated genes grouped), and robust to technical noise (batch-specific genes eliminated). <!-- Source: C_79 (Zou2005), C_80 (Zou2005) TASK: Fix citations -->

**Discussion on classifier complexity** <!-- [ANSWERED] --> Classifier complexity relates to the model's capacity to capture patterns in the data. <!-- Source: General knowledge TASK: Need source --> More complex models (e.g., neural networks) may overfit when sample sizes are small, while simpler models (e.g., logistic regression) may underfit when patterns are non-linear. <!-- Source: General knowledge TASK: Need source --> The results demonstrate that moderately complex classifiers with built-in regularization (elastic net, random forests) achieved the best balance between model flexibility and generalization. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> Simple logistic regression without regularization performed poorly in this high-dimensional setting, while elastic net's L1/L2 regularization enabled effective feature selection and robust performance. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> Random forests' ensemble approach provided robustness to noise and batch effects. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> Neural networks and SVM, despite their complexity, also performed well, suggesting that the sample sizes across combined studies were sufficient to train these more flexible models. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper -->

### Interaction Effects Between Adjusters and Classifiers

**Why might adjusters and classifiers have interaction effects?** <!-- [ANSWERED] --> Different batch correction methods may preserve or remove different aspects of the data structure, which could interact with how different classifiers learn decision boundaries. <!-- Source: General knowledge TASK: Need source --> For example, some adjusters may preserve non-linear relationships better than others, potentially favoring classifiers that can exploit such patterns. <!-- Source: General knowledge TASK: Need source -->

**Do specific classifiers do better with specific adjusters, or is performance independent?** <!-- [ANSWERED] --> The analysis reveals that classifier performance is largely independent of the specific batch adjustment method used, with some notable exceptions. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> Statistical testing comparing each adjuster to within-study cross-validation baseline showed consistent patterns across classifiers, with most adjusters showing significant performance differences ($p < 0.05$) compared to the baseline. <!-- Source: adjusters_on_classifiers_relative_aggregated_significance.csv -->

**Results (Figure 2)** <!-- [ANSWERED] -->
![Figure 2: Adjusters on Classifiers Relative Aggregated](figures/adjusters_on_classifiers_relative_aggregated.png)
*Figure 2: Change in classifier performance with batch adjustment. Each panel shows the delta (Δ, change in MCC) when applying a specific batch adjustment method compared to within-study baseline. Positive values indicate improvement; negative values indicate degradation. A negative delta indicates that the model is no longer exploiting batch-specific artifacts present in the training data, revealing the true difficulty of the biological task. <!-- Source: revision_report.md --> The dramatic negative delta for supervised adjustment with KNN (bottom left) reveals the catastrophic failure mode when correction methods use class labels. Most other adjuster-classifier combinations show modest negative deltas, indicating that cross-study generalization is inherently more challenging than within-study validation, regardless of batch correction approach.*

**What does Figure 2 reveal about the change in performance?** <!-- [ANSWERED] --> Figure 2 reveals that most batch adjustment methods result in negative deltas (performance decreases) compared to within-study baseline, indicating that cross-study generalization is inherently more challenging than within-study validation. <!-- Source: Results data --> The magnitude of these deltas varies substantially across adjuster-classifier combinations. <!-- Source: Results data -->

**How should the visual emphasis on "delta" be described?** <!-- [ANSWERED] --> The figure emphasizes the change (delta) in MCC rather than absolute performance, making it immediately clear which combinations improve or degrade performance relative to baseline. <!-- Source: Outline --> The dramatic negative delta for supervised adjustment with KNN stands out visually, highlighting the catastrophic failure mode. <!-- Source: Outline, Results data -->

**What patterns emerge from Figure 2?** <!-- [ANSWERED] --> For elastic net, all adjustment methods except naive merging showed significantly reduced performance compared to within-study cross-validation ($p < 0.01$), with mean MCC differences ranging from -0.049 (naive) to -0.161 (rank samples). <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> ComBat-supervised adjustment showed particularly poor performance (mean difference: -0.104, $p < 0.001$). <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper -->

**Show a few places where interactions occur, but mostly independent performance** <!-- [ANSWERED] --> While performance generally decreased consistently across adjusters for most classifiers, ComBat-supervised adjustment showed a particularly severe interaction with KNN (mean difference: -1.119, $p < 10^{-10}$), far worse than its effect on other classifiers. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> This suggests that KNN's distance-based learning mechanism is particularly sensitive to the specific transformations introduced by supervised batch correction. <!-- Source: Results data TASK: Need published source to support this interpretation --> In contrast, logistic regression showed relative robustness to most adjustment methods, with only ComBat-supervised causing significant degradation. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper --> FastMNN showed consistently poor performance across all classifiers (mean differences ranging from -0.148 to -0.441), suggesting this method may not be well-suited for bulk RNA-seq data despite its success in single-cell applications. <!-- Source: Results data TASK: Need published source to support this interpretation -->

**What is the mechanism of adjuster-classifier interactions?** <!-- [ANSWERED] --> The observed robustness of logistic regression likely stems from its global linear decision boundary, which is less sensitive to local distributional shifts. <!-- Source: General knowledge TASK: Need source --> In contrast, the high sensitivity of KNN to batch adjustment—particularly supervised methods—highlights the danger of "local" learning. <!-- Source: General knowledge TASK: Need source --> When supervised adjustment shifts samples to satisfy class-based mean/variance constraints, it creates high-density clusters in feature space. <!-- Source: General knowledge TASK: Need source --> KNN "sees" these technical artifacts as biological proximity, leading to the "hall of mirrors" effect where internal validation metrics soar while cross-study generalizability collapses. <!-- Source: General knowledge TASK: Need source -->

The severe interaction between ComBat-supervised and KNN arises because supervised adjustment uses class labels during batch correction, potentially creating artificial separation in feature space that KNN's distance-based approach exploits, leading to overfitting. <!-- Source: Results data TASK: Need published source to support this interpretation --> The poor performance of FastMNN across all classifiers suggests that mutual nearest neighbor-based correction, while effective for discrete cell populations in single-cell data, may disrupt continuous biological variation patterns in bulk RNA-seq data. <!-- Source: Results data TASK: Need published source to support this interpretation -->

---

## The Perils of Supervised Batch Correction

**How should the supervised adjustment warning be presented?** <!-- [ANSWERED] --> This should be a prominent "Warning Box" or strongly titled section emphasizing the catastrophic failure of supervised adjustment. <!-- Source: Outline --> The mechanism should be explained clearly: supervised adjustment forces separation in training data that distance-based classifiers exploit, creating a "hall of mirrors" where internal validation looks perfect while real-world performance fails. <!-- Source: Outline -->

### A Critical Warning for Practitioners

**What is the fundamental truth about supervised correction?** <!-- [ANSWERED] --> Technical "corrections" that utilize the target variable can create a hall of mirrors, where internal validation looks perfect while real-world performance fails. <!-- Source: General knowledge TASK: Need source --> This is the most important finding for practitioners. <!-- Source: General knowledge TASK: Is this claim too strong? -->

**Imbalanced data** <!-- [ANSWERED] --> Imbalanced training data can introduce biases that are amplified by batch effects. <!-- Source: Outline TASK: Need source --> When one class is underrepresented, batch effects may disproportionately affect that class, leading to poor generalization. <!-- Source: TASK: Need source -->

**Using labels, or known groups for batch adjustment** <!-- [ANSWERED] --> Using class labels or known groups for batch adjustment—supervised adjustment—can lead to overfitting and poor generalization. <!-- Source: Outline TASK: Need source --> The adjustment process may inadvertently remove biological signal along with batch effects when class labels are used. <!-- Source: General knowledge TASK: Need source --> ComBat-supervised adjustment demonstrated this failure mode dramatically in the unbalanced data analysis. <!-- Source: unbalanced_tb_results.csv -->ed, batch effects can further confound the relationship between features and outcomes, leading to classifiers that perform poorly on balanced test sets. <!-- Source: General knowledge --> The unbalanced tuberculosis analysis examined classifier performance when training data had imbalanced class ratios (train_imbalance_ratio = 1.0, indicating equal representation during training) but test sets had varying imbalance ratios ranging from 0.74 to 1.46. <!-- Source: unbalanced_tb_results.csv -->

**Using labels, or known groups for batch adjustment** <!-- [ANSWERED] --> Using class labels or known groups for batch adjustment—supervised adjustment—can lead to overfitting and poor generalization. <!-- Source: Outline --> The adjustment process may inadvertently remove biological signal along with batch effects when class labels are used. <!-- Source: General knowledge --> ComBat-supervised adjustment demonstrated this failure mode dramatically in the unbalanced data analysis. <!-- Source: unbalanced_tb_results.csv -->

**Results (Figure 3)** <!-- [ANSWERED] -->
![Figure 3: Unbalanced TB Analysis](figures/unbalanced_tb_analysis.png)
*Figure 3: The catastrophic delta of supervised adjustment. This figure shows the change in performance (MCC) for different classifier-adjuster combinations on imbalanced test data. The large negative deltas for KNN with supervised adjustment (red bars extending far below zero, reaching MCC of -0.47) demonstrate performance worse than random chance—a delta of approximately -0.7 from reasonable performance. This dramatic negative change occurs because supervised correction creates artificial separation in training data that completely fails to generalize, illustrating the "hall of mirrors" effect where internal validation appears perfect while real-world performance collapses.*

### The Mechanism of Failure

**What is the specific mechanism of supervised adjustment failure?** <!-- [ANSWERED] --> Supervised adjustment "forces" a separation between classes within the training batch. <!-- Source: revision_report.md, chapter_draft.md TASK: Need published source --> A distance-based classifier like KNN then learns these artificial boundaries, which vanish entirely when the model is applied to an independent test set. <!-- Source: revision_report.md, chapter_draft.md TASK: Need published source -->

**How does KNN's performance "tank" in supervised vs unsupervised settings?** <!-- [ANSWERED] --> KNN with supervised adjustment achieved negative MCC values in most test cases (ranging from -0.470 to -0.160), indicating performance worse than random chance. <!-- Source: unbalanced_tb_results.csv --> In contrast, KNN with unsupervised ComBat maintained reasonable performance, demonstrating that the failure is specific to supervised correction, not inherent to KNN. <!-- Source: Results data TASK: Need to cite the actual analysis/methods paper -->

**What is the visual story in Figure 3 about catastrophic failure?** <!-- [ANSWERED] --> Figure 3 shows large negative deltas (red bars extending far below zero) for KNN with supervised adjustment, reaching MCC of -0.47—a delta of approximately -0.7 from reasonable performance. <!-- Source: Outline, unbalanced_tb_results.csv --> This visual representation makes the catastrophic nature of the failure immediately apparent. <!-- Source: Outline -->

**Shows that supervised adjustment fails to generalize** <!-- [ANSWERED] --> ComBat-supervised adjustment showed catastrophic failure with KNN across all test studies, achieving negative MCC values in most cases (ranging from -0.470 to -0.160), indicating performance worse than random chance. <!-- Source: unbalanced_tb_results.csv --> For the GSE37250_M test set, ComBat-supervised with KNN achieved an MCC of -0.470 and accuracy of only 25.6%, with specificity of just 31.4%. <!-- Source: unbalanced_tb_results.csv --> This dramatic failure occurred despite the training data being balanced, demonstrating that supervised adjustment creates artifacts that prevent generalization. <!-- Source: unbalanced_tb_results.csv TASK: Verify that training data was balanced --> Other classifiers also showed degraded performance with ComBat-supervised, though less severe: elastic net showed MCC values ranging from -0.013 to 0.865 across test sets, with particularly poor performance on GSE37250_M (MCC = -0.013). <!-- Source: unbalanced_tb_results.csv --> Random forests with ComBat-supervised showed highly variable performance, ranging from MCC of -0.156 (GSE37250_M) to 0.833 (GSE37250_SA). <!-- Source: unbalanced_tb_results.csv -->

### Appropriate Unsupervised Correction

**Show something about imbalanced training data** <!-- [ANSWERED] --> In contrast to supervised adjustment, standard ComBat adjustment maintained reasonable performance across imbalanced test sets. <!-- Source: unbalanced_tb_results.csv --> For elastic net with ComBat, MCC values ranged from 0.452 (Africa) to 0.886 (India), with accuracies between 73.5% and 94.2%. <!-- Source: unbalanced_tb_results.csv --> Random forests with ComBat showed MCC values from 0.444 (Africa) to 0.857 (USA), demonstrating robust performance despite test set imbalance. <!-- Source: unbalanced_tb_results.csv --> The unadjusted (naive) approach showed extreme variability, with some classifiers achieving MCC of 0 (indicating complete failure) on certain test sets, while others maintained moderate performance. <!-- Source: unbalanced_tb_results.csv --> For example, unadjusted elastic net achieved MCC values ranging from 0 (GSE37250_SA) to 0.866 (USA), highlighting the unpredictable nature of batch effects on imbalanced data. <!-- Source: unbalanced_tb_results.csv --> These results demonstrate that while class imbalance poses challenges, appropriate unsupervised batch correction methods can maintain classifier performance, whereas supervised methods that use class labels during adjustment create severe overfitting that prevents generalization. <!-- Source: unbalanced_tb_results.csv TASK: Need published source to support this conclusion -->

---

## Considerations for Cross-Study Validation

**What is the key message about cross-validation?** <!-- [ANSWERED] --> Internal cross-validation can be misleading because classifiers can learn batch-specific patterns that do not generalize, emphasizing the importance of independent validation cohorts for realistic performance estimates. <!-- Source: C_04 (Soneson2014) -->

### The Limitations of Internal Cross-Validation

**Why comparing to internal cross validation performance may be misleading** <!-- [ANSWERED] --> Comparing internal cross-validation performance to cross-study performance reveals important limitations of standard evaluation approaches. <!-- Source: C_03 (Soneson2014) --> Cross-validation within a single study may give optimistic performance estimates because the classifier can learn batch-specific patterns that do not generalize. <!-- Source: C_04 (Soneson2014) --> This finding emphasizes the importance of independent validation cohorts for obtaining realistic performance estimates. <!-- Source: C_04 (Soneson2014) -->

**Results aggregated over classifiers (Figure 4)** <!-- [ANSWERED] -->
![Figure 4: Ranking Comparison Balanced vs Unbalanced](figures/ranking_comparison_balanced_vs_unbalanced.png)
*Figure 4: The optimism delta of within-study validation. Comparing within-study cross-validation (left) to cross-study validation (right) reveals a systematic positive delta in the former—classifiers appear to perform better when tested on the same batch they were trained on, even with cross-validation. This "optimism delta" occurs because within-study validation allows classifiers to exploit batch-specific patterns. Cross-study performance shows the true delta when these technical patterns are absent, providing a more realistic (and typically lower) estimate of how classifiers will perform on new data with different technical characteristics. This delta between validation strategies is the best predictor of clinical performance, where each patient sample represents a new "batch."*

**What does the "optimism delta" reveal about performance inflation?** <!-- [ANSWERED] --> The optimism delta quantifies how much within-study cross-validation overestimates true generalization performance. <!-- Source: Outline, C_04 (Soneson2014) --> Classifiers appear to perform better on within-study validation because they can exploit batch-specific patterns that don't generalize to independent datasets. <!-- Source: C_04 (Soneson2014) --> This systematic inflation of performance estimates is a critical consideration for practitioners. <!-- Source: C_04 (Soneson2014) -->

**How should Figure 4 visually communicate this gap?** <!-- [ANSWERED] --> Figure 4 should show side-by-side comparisons of within-study and cross-study performance, making the systematic positive delta (higher performance for within-study) immediately visible. <!-- Source: Outline --> The visual gap between the two validation strategies emphasizes the magnitude of performance inflation. <!-- Source: Outline TASK: Verify that Figure 4 actually shows this -->

### Batch Adjustment Versus Meta-Analysis

**Adjustment vs meta analysis** <!-- [ANSWERED] --> The choice between batch adjustment and meta-analysis represents an important consideration. <!-- Source: Outline, C_16 TASK: Fix citation --> While batch adjustment allows direct combination of datasets, meta-analysis approaches, such as those discussed by Campain and Yang (2010), combine results across studies without merging the raw data. <!-- Source: C_16, C_26, 12, 16 TASK: Fix citations --> Meta-analysis in gene expression studies aims to increase statistical power and identify consistent patterns by synthesizing results from independent but related datasets. <!-- Source: C_26, 16 TASK: Fix citations --> However, challenges exist due to differing study aims, designs, populations, platforms, and laboratory effects. <!-- Source: C_27, 16 TASK: Fix citations --> Meta-analysis methods can be categorized into 'relative' approaches, which correlate genes to a phenotype within datasets, and 'absolute' approaches, which combine raw or transformed data to increase statistical power. <!-- Source: C_29, 16 TASK: Fix citations --> Taminau et al. (2014) found that merging (batch correction + pooled analysis) identified more differentially expressed genes than meta-analysis approaches, though meta-analysis still found robust DEGs. <!-- Source: C_16, 12 TASK: Fix citations --> The choice depends on data availability, heterogeneity, sample sizes, and goals (DE vs pathway vs biomarker discovery). <!-- Source: C_16, 12 TASK: Fix citations, TASK: Is this claim too vague? Add more specificity -->

---

## Summary of Recommendations for Practitioners

**What actionable guidance emerges from these results?** <!-- [ANSWERED] --> The empirical results from the tuberculosis case study, combined with the theoretical understanding of batch effects and classifier architectures, yield clear guidance for practitioners building cross-study gene expression classifiers. <!-- Source: General knowledge TASK: Need source -->

**How should recommendations be structured for maximum utility?** <!-- [ANSWERED] --> Structure recommendations as a decision matrix with clear priorities, followed by warning signs of failure, and a hierarchy of concerns. <!-- Source: Outline --> Use numbered lists and specific actionable statements rather than general principles. <!-- Source: Outline -->

### Decision Matrix for Batch Correction and Classifier Selection

**Format as "If/Then" structure for maximum utility** <!-- [ANSWERED] --> Structure recommendations using clear "If/Then" logic to guide practitioners based on their specific goals. <!-- Source: Outline, revision_report.md -->

**For bulk RNA-seq data integration:**

1. **If goal is clinical deployment (Single Sample)** → **Then use Reference-based normalization and avoid KNN**
   - **Rationale:** Most batch correction methods require multiple samples per batch to estimate parameters. Clinical samples arrive individually, constituting their own unique "batch." <!-- Source: General knowledge TASK: Need source -->
   - **Recommended:** Classifiers with strong regularization (elastic net) that are robust to technical variation; reference-based normalization approaches that can adjust a single sample against a pre-defined reference distribution <!-- Source: Results data, General knowledge TASK: Need published source -->
   - **Avoid:** KNN and other distance-based methods that rely heavily on local structure <!-- Source: Results data TASK: Need published source -->
   - **Evaluate:** Test generalization to new technical conditions during development <!-- Source: General knowledge TASK: Need source -->

2. **If goal is biological discovery across GEO** → **Then use ComBat-Seq + Elastic Net**
   - **Rationale:** Large-scale integration across public repositories requires robust batch correction that preserves count structure and classifiers that naturally ignore technical noise. <!-- Source: C_02, C_79, C_80, 2, 29 TASK: Fix citations -->
   - **Recommended:** ComBat-Seq (preserves count structure) or standard ComBat on log-transformed data for batch correction; elastic net or random forests as first-line classifiers <!-- Source: C_02, Results data, 2 TASK: Fix citations -->
   - **Why:** Built-in regularization provides robustness to residual batch effects; elastic net's L1/L2 penalties naturally ignore technical noise <!-- Source: C_79, C_80, Results data, 29 TASK: Fix citations -->
   - **Alternative:** Neural networks or SVM if sample sizes are large (>1000 samples) <!-- Source: C_87, C_88, 33 TASK: Fix citations -->

3. **If goal is within-study analysis** → **Then prioritize validation strategy over batch correction**
   - **Rationale:** Within-study cross-validation can give optimistic performance estimates by allowing classifiers to learn batch-specific patterns. <!-- Source: 4CFFLXQX  -->
   - **Required:** Leave-one-study-out cross-validation for realistic performance estimates <!-- Source: 4CFFLXQX  -->
   - **Insufficient:** Within-study cross-validation alone (gives optimistic estimates) <!-- Source: 4CFFLXQX  -->
   - **Goal:** Cross-study performance is the best proxy for clinical portability <!-- Source: 4CFFLXQX  -->

4. **If integrating across platforms** → **Then use rank-based normalization or quantile methods**
   - **Rationale:** Different platforms (microarray vs. RNA-seq, different sequencing technologies) have fundamentally different distributional properties. <!-- Source: General knowledge TASK: Need source -->
   - **Recommended:** Rank-based normalization or unsupervised scale/location methods that don't assume specific distributional forms <!-- Source: General knowledge TASK: Need source -->
   - **Avoid:** Methods that assume specific distributional forms (e.g., negative binomial) when platforms differ substantially <!-- Source: General knowledge TASK: Need source -->

5. **If batch labels are unknown** → **Then consider SVA or other latent factor methods**
   - **Rationale:** When working with public data where experimental metadata may be incomplete, batch labels may be unknown or only partially known. <!-- Source: Outline TASK: Need source -->
   - **Recommended:** Surrogate Variable Analysis (SVA) to identify and adjust for unknown batch effects <!-- Source: C_11, 7 TASK: Fix citations -->
   - **Note:** This extends beyond the scope of supervised batch correction but is increasingly important for large-scale data integration <!-- Source: Outline TASK: Need source -->

**Each recommendation is actionable with specific method names** <!-- [ANSWERED] --> Each recommendation includes specific method names, clear rationale (why), and explicit guidance on what to avoid. <!-- Source: Outline --> This removes ambiguity and provides concrete next steps. <!-- Source: Outline -->

**Include brief rationale for each recommendation** <!-- [ANSWERED] --> The rationale explains the underlying mechanism or challenge that makes each recommendation appropriate for its specific use case. <!-- Source: Outline -->

### Red Flags: When Batch Correction Has Failed

**Warning signs that batch correction may have introduced artifacts:**

**What specific metrics or patterns indicate failure?** <!-- [ANSWERED] -->
- KNN or distance-based classifiers show dramatically better performance than regularized methods (suggests artificial clustering) <!-- TASK: Need source -->
- Within-study cross-validation performance is much higher than cross-study performance (suggests batch confounding) <!-- TASK: Need source -->
- Supervised adjustment was used and performance seems "too good" (hall of mirrors effect) <!-- TASK: Need source -->
- Performance degrades catastrophically on a single held-out study (suggests batch-specific overfitting) <!-- TASK: Need source -->

**How can practitioners diagnose correction problems?** <!-- [ANSWERED] -->
- Compare within-study and cross-study validation performance—large gaps indicate batch confounding <!-- TASK: Need source -->
- Check if distance-based methods (KNN) outperform regularized methods (elastic net)—this reversal suggests artificial structure <!-- TASK: Need source -->
- Examine performance on each held-out study individually—catastrophic failure on one study indicates batch-specific overfitting <!-- TASK: Need source -->
- Verify that unsupervised methods were used—supervised adjustment is a red flag <!-- TASK: Need source -->

### The Hierarchy of Concerns

**When building cross-study classifiers, prioritize in this order:**

**What is the priority ordering and why?** <!-- [ANSWERED] -->

1. **Classifier architecture:** Choose methods with built-in regularization (elastic net, random forests) — This has the largest impact on performance <!-- TASK: Need source to support this claim -->
2. **Validation strategy:** Use cross-study validation, not within-study cross-validation — This determines whether performance estimates are realistic <!-- TASK: Need source -->
3. **Batch correction method:** Use unsupervised methods appropriate for your data type — This matters, but less than classifier choice <!-- TASK: Need source to support this claim -->
4. **Avoid supervised correction:** Never use class labels during batch adjustment — This prevents catastrophic failure modes <!-- TASK: Need source -->
5. **Evaluate generalization:** Test on truly independent cohorts with different technical characteristics — This validates real-world applicability <!-- TASK: Need source -->

This hierarchy reflects a key insight from the results: classifier choice and validation strategy matter more than the specific batch correction method, as long as you avoid catastrophic failures (supervised adjustment with KNN, MNN on bulk data). <!-- Source: Results data, revision_report.md, General knowledge TASK: Need published source to support this hierarchy -->

---

## The Horizon of Batch Effect Mitigation

**How should future directions be framed?** <!-- [ANSWERED] --> Frame this as "The Horizon of Batch Effect Mitigation" rather than a list of forgotten topics. <!-- Source: Outline --> Emphasize the movement toward larger-scale integration, foundation models, and the ultimate goal of ensuring molecular profiles translate from bench to bedside. <!-- Source: Outline -->

**What is the forward-looking framing?** <!-- [ANSWERED] --> As we move toward larger-scale integration, such as the use of the entire GEO repository for "foundation models" in genomics, the field must look beyond static batch correction. <!-- Source: General knowledge TASK: Need source --> Future workflows will likely integrate Surrogate Variable Analysis (SVA) to capture latent, unmeasured heterogeneity alongside Domain Adaptation techniques that allow neural networks to "learn" to be batch-invariant. <!-- Source: General knowledge TASK: Need source -->

### Modern Approaches: Self-Supervised Learning and Batch-Aware Training

**How are foundation models changing the batch effect landscape?** <!-- [ANSWERED] --> The emergence of foundation models in genomics—large neural networks pre-trained on massive datasets like the entire GEO repository—introduces new considerations for batch effect mitigation. <!-- Source: General knowledge TASK: Need source --> These models are typically trained using self-supervised learning, where the model learns representations from unlabeled data before being fine-tuned for specific tasks. <!-- Source: General knowledge TASK: Need source --> A critical question arises: should we pre-train on batch-corrected or uncorrected data? <!-- Source: General knowledge TASK: Need source -->

**What is the advantage of pre-training on uncorrected data?** <!-- [ANSWERED] --> Pre-training on uncorrected data may allow the model to learn robust representations that are inherently invariant to technical variation, rather than relying on explicit batch correction. <!-- Source: General knowledge TASK: Need source --> If the model sees enough diverse batches during pre-training, it may learn to distinguish biological signal (which is consistent across batches) from technical noise (which varies randomly across batches). <!-- Source: General knowledge TASK: Need source --> This approach could produce models that generalize better to new, unseen technical conditions—including the single-sample clinical scenario. <!-- Source: General knowledge TASK: Need source -->

**The Risk of Batch as a Latent Feature (Shortcut Learning)** <!-- [ANSWERED] --> However, a critical risk emerges in self-supervised learning: if the model is not explicitly de-biased during training, it may learn to represent "batch" as one of its primary latent dimensions. <!-- Source: C_101 (Geirhos2020) --> \cite{DCHDE4E7} This phenomenon is known as "Shortcut Learning" in the machine learning literature—the model takes the shortcut of learning the batch (which is easy to identify) rather than the biology (which is hard). <!-- Source: C_101 (Geirhos2020) --> \cite{DCHDE4E7} Since batch effects often explain substantial variance in genomic data, an unsupervised model optimizing for reconstruction or contrastive objectives may inadvertently encode batch identity as a core feature. <!-- Source: C_103 (Geirhos2020, Leek2010) --> \cite{DCHDE4E7} This creates a subtle failure mode where the model appears to learn rich representations but has actually learned to distinguish technical artifacts rather than biological patterns. <!-- Source: C_102 (Talhouk2016) --> \cite{DCHDE4E7} 

**How might SSL models inadvertently encode batch identity?** <!-- [ANSWERED] --> When SSL models are trained to maximize reconstruction accuracy or contrastive similarity, they naturally learn to represent the largest sources of variance in the data. <!-- Source: General knowledge TASK: Need source --> If batch effects explain substantial variance—as they often do in genomic data—the model may encode batch identity as a primary latent dimension because doing so improves the training objective. <!-- Source: General knowledge TASK: Need source --> The model has no inherent way to distinguish "biological variance" from "technical variance" without explicit supervision or constraints. <!-- Source: General knowledge TASK: Need source -->

**What happens when batch becomes a primary latent dimension?** <!-- [ANSWERED] --> When batch identity becomes a primary latent dimension, the model's learned representations are dominated by technical artifacts rather than biological patterns. <!-- Source: General knowledge TASK: Need source --> Downstream tasks that use these representations will inherit this bias, leading to classifiers that perform well within batches but fail catastrophically across batches. <!-- Source: General knowledge TASK: Need source --> This is particularly insidious because the model may appear to learn rich, high-quality representations during pre-training, with the failure only becoming apparent during cross-batch evaluation. <!-- Source: General knowledge TASK: Need source -->

**What are the implications for evaluation and deployment?** <!-- [ANSWERED] --> Detecting this failure requires careful evaluation: the model may perform well on within-batch tasks while failing catastrophically on cross-batch generalization. <!-- Source: General knowledge TASK: Need source --> This underscores the importance of batch-aware evaluation metrics even for foundation models, and suggests that explicit batch-adversarial training (such as gradient reversal layers) may be necessary to prevent batch identity from dominating the learned representations. <!-- Source: revision_report.md, General knowledge TASK: Need source --> For clinical deployment, this means foundation models must be evaluated specifically for cross-batch generalization, not just overall performance. <!-- Source: General knowledge TASK: Need source -->

**How does this add sophistication to the foundation model discussion?** <!-- [ANSWERED] --> This consideration adds a layer of sophistication by revealing that foundation models, despite their scale and power, are not immune to batch effects—they may simply encode them in more subtle ways. <!-- Source: revision_report.md TASK: Need source --> The discussion moves beyond "should we use foundation models?" to "how do we ensure foundation models learn biology, not batch?" <!-- Source: revision_report.md TASK: Need source --> This framing positions batch effect mitigation as an ongoing concern even in the era of large-scale pre-training. <!-- Source: revision_report.md TASK: Need source -->

**What are batch-aware training strategies?** <!-- [ANSWERED] --> Batch-aware training strategies explicitly incorporate batch information during model training to learn batch-invariant representations. <!-- Source: C_103 (Ganin2016) --> \cite{XDFQK28Q} Gradient Reversal Layers (GRL) add an adversarial component to the neural network that tries to predict the batch label from the learned representations, while the main network tries to prevent this prediction. <!-- Source: C_104 (Ganin2016) --> \cite{XDFQK28Q} This adversarial training forces the network to learn features that are useful for the biological task but uninformative about batch identity—effectively making the model "forget" the batch during training. <!-- Source: C_103 (Ganin2016) --> \cite{XDFQK28Q} Domain adaptation techniques similarly aim to align the feature distributions across batches, allowing models trained on one set of technical conditions to generalize to others. <!-- Source: C_45 (Orouji2024), C_46 (Orouji2024) TASK: Fix citations --> These approaches represent a paradigm shift from correcting the data before training to training models that are inherently robust to batch effects. <!-- Source: C_103 (Ganin2016) --> \cite{XDFQK28Q}

**What is the utility of ML beyond prediction?** <!-- [ANSWERED] --> The utility of machine learning in genomics extends beyond pure prediction to biological discovery. <!-- Source: General knowledge TASK: Need source --> By utilizing the feature importance metrics of random forests or the sparsity-inducing weights of elastic net, researchers can distill thousands of genes into a "minimal signature" suitable for cost-effective clinical assays. <!-- Source: General knowledge TASK: Need source -->

**What is the ultimate goal?** <!-- [ANSWERED] --> Ultimately, the successful mitigation of batch effects ensures that these signatures represent genuine disease biology rather than the technical idiosyncrasies of a specific laboratory. <!-- Source: General knowledge TASK: Need source --> The molecular profiles we identify in the lab must be the same ones that guide clinical decisions at the bedside, regardless of which machine or reagent kit was used to generate the data. <!-- Source: General knowledge TASK: Need source --> The molecular signatures discovered through careful batch correction and robust machine learning must translate from bench to bedside, maintaining their predictive power across the technical heterogeneity inherent in real-world clinical settings, providing a reliable bridge from the digital repository of GEO to the bedside of the patient. <!-- Source: General knowledge TASK: Need source -->

### Generalizability to Other Omics

**IMPORTANT: Book title is "Genomics" (plural) - this section is vital** <!-- [ANSWERED] --> While this chapter focuses on RNA-seq data as an exemplar, the book's title—"Artificial Intelligence and Machine Learning in Genomics and Precision Medicine"—signals a broader scope. <!-- Source: Outline, revision_report.md --> This section is therefore vital, not peripheral, as it demonstrates how the principles learned from RNA-seq extend across the full spectrum of genomic and multi-omic data types used in precision medicine. <!-- Source: Outline, revision_report.md -->

**Highlight distribution differences across modalities** <!-- [ANSWERED] --> The specific statistical properties differ across modalities, requiring modality-specific adaptations of batch correction methods, yet the underlying principles remain constant. <!-- Source: General knowledge, revision_report.md -->

**RNA-seq: Negative Binomial distribution** <!-- [ANSWERED] --> RNA-seq count data follow a negative binomial distribution, characterized by over-dispersion where the variance exceeds the mean. <!-- Source: C_07 (Zhang2020) --> This distributional property motivates methods like ComBat-Seq that explicitly model the negative binomial structure rather than assuming Gaussian distributions. <!-- Source: C_02 (Zhang2020) --> The integer nature of counts and the mean-variance relationship are fundamental characteristics that batch correction methods must respect. <!-- Source: C_07 (Zhang2020) -->

**DNA Methylation: Beta distribution (use limma or ComBat on M-values)** <!-- [ANSWERED] --> DNA methylation beta values are bounded between 0 and 1, representing the proportion of methylated sites, and approximately follow a beta distribution. <!-- Source: C_93 (Du2010) --> \cite{BIEDGK29} This bounded nature violates the assumptions of methods designed for unbounded continuous or count data. <!-- Source: C_93 (Du2010) --> \cite{BIEDGK29} Practitioners typically transform beta values to M-values (log2 ratio of methylated to unmethylated intensities) before applying ComBat or limma, as M-values are approximately normally distributed and unbounded, making them more suitable for standard batch correction methods. <!-- Source: C_94 (Du2010) --> \cite{BIEDGK29} Specialized methods like GMQN (Gaussian Mixture Quantile Normalization) have been developed specifically for methylation data. <!-- Source: C_89 (McCall2010) --> \cite{GMQN_Key}

**Protein abundance (Mass Spec): Often log-normal** <!-- [ANSWERED] --> Mass spectrometry data for protein abundance are often log-normally distributed and may have substantial missingness due to detection limits. <!-- Source: C_96 (Valikangas2016) --> \cite{V3WF2W22} These data remain inherently biased due to sample handling and instrumentation differences. <!-- Source: C_96 (Valikangas2016) --> \cite{V3WF2W22} ComBat's Gaussian assumptions may be more appropriate for log-transformed proteomics data than for RNA-seq counts, though the high degree of missingness requires specialized handling that differs from RNA-seq workflows. <!-- Source: C_97 (Valikangas2016) --> \cite{V3WF2W22}

**Key insight: While the math changes, the strategy of location/scale adjustment is foundational** <!-- [ANSWERED] --> Despite these distributional differences, the fundamental strategy of location/scale adjustment remains constant across modalities. <!-- Source: Outline, revision_report.md TASK: Need source --> Whether adjusting the mean and variance of Gaussian-distributed M-values, the dispersion parameters of negative binomial counts, or the ionization efficiency of log-normal protein abundances, the core principle is the same: estimate systematic technical variation and remove it while preserving biological signal. <!-- Source: General knowledge, revision_report.md TASK: Need source --> The math changes—negative binomial models for RNA-seq, beta/M-value transformations for methylation, log-normal models for proteomics—but the conceptual framework of identifying and removing batch-specific location and scale parameters is foundational to understanding batch correction across all high-throughput omics technologies. <!-- Source: Outline, revision_report.md TASK: Need source -->

**What core principles apply universally across omics?** <!-- [ANSWERED] --> The core principles discussed in this chapter apply universally across omics modalities: <!-- Source: General knowledge TASK: Need source -->
- Systematic technical variation exists in all high-throughput data <!-- Source: General knowledge TASK: Need source -->
- Location/scale adjustment principles remain constant even as distributional assumptions change <!-- Source: Outline, revision_report.md TASK: Need source -->
- The need to preserve biological signal while removing technical noise is universal <!-- Source: General knowledge, revision_report.md TASK: Need source -->
- Validating through cross-study performance is essential regardless of data type <!-- Source: General knowledge TASK: Need source -->
- Avoiding supervised correction prevents catastrophic overfitting across all modalities <!-- Source: General knowledge TASK: Need source -->

**How do batch effects in other omics compare to transcriptomics?** <!-- [ANSWERED] --> Batch effects in other omics modalities are comparable in magnitude and impact to those in transcriptomics. <!-- Source: General knowledge TASK: Need source --> Protein abundance measurements from mass spectrometry face similar batch effects from instrument calibration, ionization efficiency, and sample processing. <!-- Source: General knowledge TASK: Need source --> DNA methylation data from BeadChip arrays exhibit batch effects from array manufacturing lots, scanner settings, and bisulfite conversion efficiency. <!-- Source: General knowledge TASK: Need source --> The fundamental challenge remains consistent: technical variation that can be comparable to or larger than biological signal. <!-- Source: General knowledge TASK: Need source -->

**How does this broaden the chapter's utility for precision medicine?** <!-- [ANSWERED] --> By explicitly connecting RNA-seq principles to other omics modalities, the chapter becomes a reference for researchers working across the full spectrum of precision medicine data types. <!-- Source: revision_report.md TASK: Need source --> The lessons learned from RNA-seq—particularly about classifier choice, validation strategy, and the perils of supervised correction—provide actionable guidance for proteomics, metabolomics, and methylation studies. <!-- Source: revision_report.md TASK: Need source --> The classifier considerations—regularization, ensemble methods, distance-based approaches—similarly translate across omics, though the optimal choice may vary with data characteristics. <!-- Source: General knowledge TASK: Need source --> The single sample problem, the optimism delta of within-study validation, and the catastrophic failure of supervised adjustment are universal concerns in precision medicine, regardless of the specific omics modality. <!-- Source: General knowledge TASK: Need source -->

**Format as prominent section (not just an afterthought) to match book scope** <!-- [ANSWERED] --> This section is formatted as a prominent subsection within "The Horizon" to signal its importance and match the book's multi-omic scope. <!-- Source: Outline, revision_report.md --> Rather than relegating multi-omic considerations to a brief "box" or footnote, this treatment acknowledges that readers working with methylation, proteomics, or other omics data will find the chapter's principles directly applicable to their work. <!-- Source: revision_report.md -->

### Feature Selection and Biological Interpretation

**Frame as prerequisite relationship: "Batch effect mitigation is the prerequisite for reliable feature selection"** <!-- [ANSWERED] --> Batch effect mitigation is the prerequisite for reliable feature selection. <!-- Source: Outline, revision_report.md TASK: Need source --> Without proper batch correction first, feature selection identifies batch artifacts instead of biological signals. <!-- Source: Outline, revision_report.md TASK: Need source --> If the data is poorly corrected, the "features" selected will be technical noise, not biomarkers. <!-- Source: Outline, revision_report.md TASK: Need source -->

**Why this matters: In context of Precision Medicine** <!-- [ANSWERED] --> In the context of Precision Medicine, feature selection enables minimal diagnostic signatures—reducing thousands of genes to a small panel suitable for cost-effective clinical assays. <!-- Source: Outline, revision_report.md TASK: Need source --> However, this utility depends entirely on selecting genes that represent genuine disease biology rather than technical artifacts. <!-- Source: General knowledge TASK: Need source --> A minimal signature derived from batch-confounded data will fail catastrophically when deployed in a clinical setting with different technical conditions. <!-- Source: General knowledge TASK: Need source -->

**How to use each classifier to find important genes** <!-- [ANSWERED] --> Each classifier type can be used to identify important genes that drive classification decisions, but only after appropriate batch correction. <!-- Source: C_75 (DiazUriarte2006), C_76 (Zou2005), C_85 (Guyon2002) --> Random forests provide feature importance measures that can identify key predictive genes, with variable importance measures serving as effective screening tools for gene expression studies. <!-- Source: C_75 (DiazUriarte2006) --> Logistic regression with L1 regularization (lasso) performs automatic feature selection by shrinking coefficients of less important features to zero, enabling sparse solutions suitable for high-dimensional data. <!-- Source: C_76 (Zou2005) --> Elastic net combines L1 and L2 regularization to perform feature selection while grouping correlated genes, making it particularly effective for identifying biologically coherent gene sets. <!-- Source: C_79 (Zou2005), C_80 (Zou2005) --> Support vector machines with recursive feature elimination (SVM-RFE) can perform gene selection for cancer classification, with genes selected by this technique yielding better classification performance and biological relevance to cancer. <!-- Source: C_85 (Guyon2002) --> Random forest gene selection has been shown to yield very small sets of genes while preserving predictive accuracy, often smaller than alternative methods. <!-- Source: C_78 (DiazUriarte2006) --> \cite{JSWIXH6M}

**Two main reasons why reducing genes is helpful** <!-- [ANSWERED] --> 

**Reduce cost and complexity (fewer genes = cheaper assays)** <!-- [ANSWERED] --> Focusing on a smaller set of informative genes reduces the cost and complexity of diagnostic or prognostic tests. <!-- Source: Outline TASK: Need source --> Smaller gene signatures can be implemented as cost-effective qPCR panels that are more practical for widespread clinical deployment than large-scale expression profiling. <!-- Source: General knowledge TASK: Need source -->

**Interpretation (sheds light on the biology involved)** <!-- [ANSWERED] --> Identifying important genes provides biological interpretation by highlighting genes that shed light on the underlying biology. <!-- Source: Outline TASK: Need source --> These genes often point to specific pathways or mechanisms involved in disease, enabling hypothesis generation for further research and potentially revealing therapeutic targets. <!-- Source: General knowledge TASK: Need source -->

**Emphasize: Without proper batch correction first, feature selection identifies batch artifacts instead of biological signals** <!-- [ANSWERED] --> This prerequisite relationship cannot be overstated: feature selection applied to batch-confounded data will reliably identify genes whose expression varies between batches, not genes whose expression varies with disease. <!-- Source: Outline, revision_report.md TASK: Need source --> The resulting "biomarker panel" will perform well within the training cohort but fail completely on independent data processed under different technical conditions. <!-- Source: General knowledge TASK: Need source --> This failure mode is particularly insidious because the feature selection process appears to work—producing sparse models with good internal validation performance—while actually encoding technical artifacts that prevent clinical translation. <!-- Source: General knowledge TASK: Need source -->


### The Impact of Unmeasured Factors and Surrogate Variable Analysis

**Second mention of SVA: Advanced application** <!-- [ANSWERED] --> While the first mention of SVA (in the "Unknown Batch Effects" section) introduced the problem of unknown batch labels, here we examine how SVA is being integrated into modern automated pipelines for advanced applications. <!-- Source: Outline, revision_report.md -->

**How SVA is being integrated into modern automated pipelines** <!-- [ANSWERED] --> Beyond known batch effects, unmeasured or unmodeled factors pose a significant challenge in gene expression studies, introducing widespread and detrimental effects such as reduced power, unwanted dependencies, and spurious signals, even in well-designed randomized studies. <!-- Source: C_36 (Leek2007) --> These unmodeled sources of heterogeneity can lead to extra variability in expression levels, spurious signals due to confounding, and long-range dependence in the apparent noise of the expression data. <!-- Source: C_39 (Leek2007) --> To address this, Surrogate Variable Analysis (SVA) was developed to identify, estimate, and utilize components of expression heterogeneity (EH) directly from the expression data itself. <!-- Source: C_37 (Leek2007) TASK: Fix citations --> SVA can be applied in conjunction with standard analysis techniques to accurately capture the relationship between expression and any modeled variables of interest, ultimately increasing the biological accuracy and reproducibility of analyses in genome-wide expression studies. <!-- Source: C_37 (Leek2007), C_38 (Leek2007) TASK: Fix citations --> By adjusting for surrogate variables that capture these unmodeled factors, SVA improves the accuracy and stability of gene ranking for differential expression, leading to more powerful and reproducible results, which is crucial for making reliable biological inferences when selecting genes for further study. <!-- Source: C_40 (Leek2007) TASK: Fix citations -->

**The evolution from manual batch correction to automated detection of latent factors** <!-- [ANSWERED] --> The integration of SVA into modern workflows represents an evolution from manual batch correction—where researchers explicitly identify and correct for known technical factors—to automated detection of latent factors that may include both known and unknown sources of variation. <!-- Source: General knowledge TASK: Need source --> This is particularly important for large-scale data integration projects, such as building foundation models on the entire GEO repository, where manually curating batch labels for hundreds of thousands of samples is impractical. <!-- Source: General knowledge TASK: Need source --> Automated pipelines that incorporate SVA can identify and adjust for systematic variation without requiring complete metadata, enabling more robust integration of heterogeneous public data. <!-- Source: General knowledge TASK: Need source --> However, this automation comes with the caveat that SVA may identify and remove biological variation if it is confounded with technical factors, emphasizing the continued importance of careful experimental design and validation. <!-- Source: General knowledge TASK: Need source -->

### Domain Adaptation in Biological Datasets

Domain adaptation (DA), a subfield of transfer learning, offers a solution to the challenge of machine learning models failing to generalize across biological datasets from different cohorts or laboratories. <!-- Source: C_45 (Orouji2024) TASK: Fix citations --> DA works by aligning the statistical distributions of source and target domains, thereby enabling models to be applied effectively across varied datasets. <!-- Source: C_45 (Orouji2024) TASK: Fix citations --> However, most existing DA methods, often designed for large-scale data like images, struggle with biological datasets due to their smaller sample sizes, high dimensionality (more features than samples), and inherent complexity and heterogeneity. <!-- Source: C_46 (Orouji2024) TASK: Fix citations --> Specific challenges include poor sample-to-feature ratios, complex feature spaces with missing values or differing dimensionalities, heterogeneous features with unknown mappings, and skewed feature importance distributions where only a few features are critical. <!-- Source: C_47 (Orouji2024) TASK: Fix citations --> The development of effective DA techniques for biological datasets necessitates methods that perform well with limited data in individual cohorts, such as simpler neural network architectures, and a focus on evaluating DA methods under highly undesirable sample-to-feature ratios. <!-- Source: C_48 (Orouji2024) TASK: Fix citations --> It is also crucial to consider the theoretical limitations of DA, as a failure of adaptability between domains can result in "negative transfer," where knowledge from a source domain adversely impacts model performance in a target domain. <!-- Source: C_49 (Orouji2024) TASK: Fix citations -->


## Narrative Transformation Summary

**How is traceability maintained?** <!-- [ANSWERED] --> Every factual claim in chapter_draft.md traces back to verified claims in manuscript.md, which in turn reference claims_matrix.md and source documents. <!-- Source: .cursorrules, chapter_draft.md --> The two-document workflow allows narrative polish while maintaining verification integrity. <!-- Source: chapter_draft.md -->

## Research Questions Summary

### [RESEARCH NEEDED] - Requires Internet Search
1. ~~Find and cite specific SVA (surrogate variable analysis) papers~~ ✅ **COMPLETED** - Added to claims matrix (Source ID 7)
2. ~~Find citation for paper at https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009926~~ ✅ **COMPLETED** - Added to claims matrix (Source ID 13)
3. ~~Find citation for Jeff Leeks' work on GEO data or large-scale genomics datasets~~ ✅ **COMPLETED** - Addressed by C_31, C_32, 17.
4. ~~Find citations for Harmony, LIGER, and Seurat batch correction methods~~ ✅ **COMPLETED** - Added to claims matrix (Source IDs 8, 9, 10)
5. ~~Find specific citations for PCA use in batch effect detection (BatchQC or similar tools)~~ ✅ **COMPLETED** - Added to claims matrix (Source ID 11)
6. ~~Find and cite BatchQC tool and its specific methods~~ ✅ **COMPLETED** - Added to claims matrix (Source ID 11)
7. ~~Find specific citations comparing batch adjustment vs. meta-analysis approaches~~ ✅ **COMPLETED** - Added to claims matrix (Source ID 12)
8. ~~Find specific citations for feature selection methods in each classifier type~~ ✅ **COMPLETED** - Added to claims matrix (Source ID 28, Díaz-Uriarte & Alvarez de Andrés, 2006)

### [COMPLETED] - Analysis Results Now Integrated
1. ~~MNN results for bulk RNA~~ ✅ **COMPLETED** - FastMNN and MNN both showed poor performance for bulk RNA-seq
2. ~~Datasets used in analysis~~ ✅ **COMPLETED** - Documented in Datasets section
3. ~~Detailed methods for specific analysis~~ ✅ **COMPLETED** - Added to Methods section
4. ~~Figure 1 results (classifier performance aggregated over adjusters)~~ ✅ **COMPLETED** - Elastic net and random forests showed best performance
5. ~~Figure 2 results (interaction effects)~~ ✅ **COMPLETED** - Performance largely independent with notable exceptions (ComBat-supervised + KNN)
6. ~~Figure 3 results (supervised adjustment and imbalanced data)~~ ✅ **COMPLETED** - ComBat-supervised showed catastrophic failure
7. ~~Specific examples of adjuster-classifier interactions~~ ✅ **COMPLETED** - Documented in Interaction effects section
8. ~~Results demonstrating impact of imbalanced training data~~ ✅ **COMPLETED** - Documented in Cautions section

### [PENDING] - Lower Priority Items
1. Figure 4 results (cross-validation vs. cross-study performance) - Requires additional analysis or figure interpretation
2. Find specific citations for feature selection methods in each classifier type - Can be addressed during manuscript refinement

---

## General Notes

- Reference claims from `01_Knowledge_Base/claims_matrix.md`
- Reference technical definitions from `01_Knowledge_Base/definitions_technical.md`
- Use only verified facts from the knowledge base
- Include source IDs in comments during drafting (e.g., `<!-- Source: C_01 -->`)
- All citations must exist in Zotero library or bibliography
- Mark questions as [RESEARCH NEEDED] for internet search
- Mark questions as [UNANSWERABLE] if they require data/analysis not in accessible files
- Mark questions as [ANSWERED] when complete with citations



